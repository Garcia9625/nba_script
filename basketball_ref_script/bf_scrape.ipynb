{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7a2396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Parsed 655 entries â†’ valid 655, duplicates 86, invalid 0.\n",
      "ðŸ“„ Using 569 unique URL(s) from data/players.txt\n",
      "âš ï¸ Duplicate entries (player appeared multiple times):\n",
      "   - https://www.basketball-reference.com/players/c/craigto01.html\n",
      "   - https://www.basketball-reference.com/players/h/huntede01.html\n",
      "   - https://www.basketball-reference.com/players/l/leverca01.html\n",
      "   - https://www.basketball-reference.com/players/n/niangge01.html\n",
      "   - https://www.basketball-reference.com/players/m/moorewe01.html\n",
      "   - https://www.basketball-reference.com/players/s/schrode01.html\n",
      "   - https://www.basketball-reference.com/players/b/beekmre01.html\n",
      "   - https://www.basketball-reference.com/players/s/schrode01.html\n",
      "   - https://www.basketball-reference.com/players/w/waterli01.html\n",
      "   - https://www.basketball-reference.com/players/r/roddyda01.html\n",
      "   - https://www.basketball-reference.com/players/b/brownmo01.html\n",
      "   - https://www.basketball-reference.com/players/b/bogdabo01.html\n",
      "   - https://www.basketball-reference.com/players/j/joneska01.html\n",
      "   - https://www.basketball-reference.com/players/m/mannte01.html\n",
      "   - https://www.basketball-reference.com/players/s/simmobe01.html\n",
      "   - https://www.basketball-reference.com/players/c/chrisma02.html\n",
      "   - https://www.basketball-reference.com/players/d/davisan02.html\n",
      "   - https://www.basketball-reference.com/players/d/doncilu01.html\n",
      "   - https://www.basketball-reference.com/players/f/finnedo01.html\n",
      "   - https://www.basketball-reference.com/players/k/klebima01.html\n",
      "   ... and 66 more\n",
      "[1/569] âœ… barlodo01 â€” written\n",
      "[2/569] âœ… bogdabo01 â€” written\n",
      "[3/569] âœ… bufkiko01 â€” written\n",
      "[4/569] âœ… capelca01 â€” written\n",
      "[5/569] âœ… daniedy01 â€” written\n",
      "[6/569] âœ… gueyemo02 â€” written\n",
      "[7/569] âœ… huntede01 â€” written\n",
      "[8/569] âœ… johnsja05 â€” written\n",
      "[9/569] âœ… krejcvi01 â€” written\n",
      "[10/569] âœ… leverca01 â€” written\n",
      "[11/569] âœ… mannte01 â€” written\n",
      "[12/569] âœ… mathega01 â€” written\n",
      "[13/569] âœ… nancela02 â€” written\n",
      "[14/569] âœ… niangge01 â€” written\n",
      "[15/569] âœ… okongon01 â€” written\n",
      "[16/569] âœ… plowdda01 â€” written\n",
      "[17/569] âœ… risacza01 â€” written\n",
      "[18/569] âœ… roddyda01 â€” written\n",
      "[19/569] âœ… toppija01 â€” written\n",
      "[20/569] âœ… wallake01 â€” written\n",
      "[21/569] âœ… youngtr01 â€” written\n",
      "[22/569] âœ… brownja02 â€” written\n",
      "[23/569] âœ… craigto01 â€” written\n",
      "[24/569] âœ… davisjd01 â€” written\n",
      "[25/569] âœ… hausesa01 â€” written\n",
      "[26/569] âœ… holidjr01 â€” written\n",
      "[27/569] âœ… horfoal01 â€” written\n",
      "[28/569] âœ… kornelu01 â€” written\n",
      "[29/569] âœ… norrimi01 â€” written\n",
      "[30/569] âœ… peterdr01 â€” written\n",
      "[31/569] âœ… porzikr01 â€” written\n",
      "[32/569] âœ… pritcpa01 â€” written\n",
      "[33/569] âœ… quetane01 â€” written\n",
      "[34/569] âœ… scheiba01 â€” written\n",
      "[35/569] âœ… sprinja01 â€” written\n",
      "[36/569] âœ… tatumja01 â€” written\n",
      "[37/569] âœ… tillmxa01 â€” written\n",
      "[38/569] âœ… walshjo01 â€” written\n",
      "[39/569] âœ… whitede01 â€” written\n",
      "[40/569] âœ… beekmre01 â€” written\n",
      "[41/569] âœ… claxtni01 â€” written\n",
      "[42/569] âœ… clownno01 â€” written\n",
      "[43/569] âœ… cuiyo01 â€” written\n",
      "[44/569] âœ… etienty01 â€” written\n",
      "[45/569] âœ… evbuoto01 â€” written\n",
      "[46/569] âœ… finnedo01 â€” written\n",
      "[47/569] âœ… hayeski01 â€” written\n",
      "[48/569] âœ… johnsca02 â€” written\n",
      "[49/569] âœ… johnske07 â€” written\n",
      "[50/569] âœ… lewisma05 â€” written\n",
      "[51/569] âœ… martija02 â€” written\n",
      "[52/569] âœ… martity01 â€” written\n",
      "[53/569] âœ… whiteda01 â€” written\n",
      "[54/569] âœ… miltosh01 â€” written\n",
      "[55/569] âœ… russeda01 â€” written\n",
      "[56/569] âœ… schrode01 â€” written\n",
      "[57/569] âœ… sharpda01 â€” written\n",
      "[58/569] âœ… simmobe01 â€” written\n",
      "[59/569] âœ… thomaca02 â€” written\n",
      "[60/569] âœ… timmedr01 â€” written\n",
      "[61/569] âœ… watfotr01 â€” written\n",
      "[62/569] âœ… willizi02 â€” written\n",
      "[63/569] âœ… wilsoja03 â€” written\n",
      "[64/569] âœ… balllo01 â€” written\n",
      "[65/569] âœ… buzelma01 â€” written\n",
      "[66/569] âœ… carteje01 â€” written\n",
      "[67/569] âœ… colliza01 â€” written\n",
      "[68/569] âœ… dosunay01 â€” written\n",
      "[69/569] âœ… duartch01 â€” written\n",
      "[70/569] âœ… giddejo01 â€” written\n",
      "[71/569] âœ… hortota01 â€” written\n",
      "[72/569] âœ… huertke01 â€” written\n",
      "[73/569] âœ… jonestr01 â€” written\n",
      "[74/569] âœ… lavinza01 â€” written\n",
      "[75/569] âœ… liddeej01 â€” written\n",
      "[76/569] âœ… milleem01 â€” written\n",
      "[77/569] âœ… phillju01 â€” written\n",
      "[78/569] âœ… sanogad01 â€” written\n",
      "[79/569] âœ… smithja04 â€” written\n",
      "[80/569] âœ… terryda01 â€” written\n",
      "[81/569] âœ… vucevni01 â€” written\n",
      "[82/569] âœ… whiteco01 â€” written\n",
      "[83/569] âœ… willipa01 â€” written\n",
      "[84/569] âœ… youngja05 â€” written\n",
      "[85/569] âœ… ballla01 â€” written\n",
      "[86/569] âœ… baughda01 â€” written\n",
      "[87/569] âœ… bridgmi02 â€” written\n",
      "[88/569] âœ… curryse01 â€” written\n",
      "[89/569] âœ… diabamo01 â€” written\n",
      "[90/569] âœ… flynnma01 â€” written\n",
      "[91/569] âœ… garrema01 â€” written\n",
      "[92/569] âœ… gibsota01 â€” written\n",
      "[93/569] âœ… greenjo02 â€” written\n",
      "[94/569] âœ… jeffrda01 â€” written\n",
      "[95/569] âœ… manntr01 â€” written\n",
      "[96/569] âœ… martico01 â€” written\n",
      "[97/569] âœ… micicva01 â€” written\n",
      "[98/569] âœ… millebr02 â€” written\n",
      "[99/569] âœ… moorewe01 â€” written\n",
      "[100/569] âœ… nurkiju01 â€” written\n",
      "[101/569] âœ… okogijo01 â€” written\n",
      "[102/569] âœ… paytoel01 â€” written\n",
      "[103/569] âœ… rhodeja01 â€” written\n",
      "[104/569] âœ… richani01 â€” written\n",
      "[105/569] âœ… salauti01 â€” written\n",
      "[106/569] âœ… simpskj01 â€” written\n",
      "[107/569] âœ… simsja01 â€” written\n",
      "[108/569] âœ… smithni01 â€” written\n",
      "[109/569] âœ… willigr01 â€” written\n",
      "[110/569] âœ… willima07 â€” written\n",
      "[111/569] âœ… wongis01 â€” written\n",
      "[112/569] âœ… allenja01 â€” written\n",
      "[113/569] âœ… batesem01 â€” written\n",
      "[114/569] âœ… garlada01 â€” written\n",
      "[115/569] âœ… greenja02 â€” written\n",
      "[116/569] âœ… jeromty01 â€” written\n",
      "[117/569] âœ… merrisa01 â€” written\n",
      "[118/569] âœ… mitchdo01 â€” written\n",
      "[119/569] âœ… mobleev01 â€” written\n",
      "[120/569] âœ… okekech01 â€” written\n",
      "[121/569] âœ… okorois01 â€” written\n",
      "[122/569] âœ… portecr01 â€” written\n",
      "[123/569] âœ… strusma01 â€” written\n",
      "[124/569] âœ… thomptr01 â€” written\n",
      "[125/569] âœ… thorjt01 â€” written\n",
      "[126/569] âœ… tomlina01 â€” written\n",
      "[127/569] âœ… travelu01 â€” written\n",
      "[128/569] âœ… tysonja01 â€” written\n",
      "[129/569] âœ… wadede01 â€” written\n",
      "[130/569] âœ… brownmo01 â€” written\n",
      "[131/569] âœ… chrisma02 â€” written\n",
      "[132/569] âœ… davisan02 â€” written\n",
      "[133/569] âœ… dinwisp01 â€” written\n",
      "[134/569] âœ… doncilu01 â€” written\n",
      "[135/569] âœ… edwarke02 â€” written\n",
      "[136/569] âœ… exumda01 â€” written\n",
      "[137/569] âœ… gaffoda01 â€” written\n",
      "[138/569] âœ… gortmja01 â€” written\n",
      "[139/569] âœ… grimequ01 â€” written\n",
      "[140/569] âœ… hardyja02 â€” written\n",
      "[141/569] âœ… irvinky01 â€” written\n",
      "[142/569] âœ… joneska01 â€” written\n",
      "[143/569] âœ… kelleky01 â€” written\n",
      "[144/569] âœ… klebima01 â€” written\n",
      "[145/569] âœ… livelde01 â€” written\n",
      "[146/569] âœ… marshna01 â€” written\n",
      "[147/569] âœ… martica02 â€” written\n",
      "[148/569] âœ… morrima02 â€” written\n",
      "[149/569] âœ… poweldw01 â€” written\n",
      "[150/569] âœ… prospol01 â€” written\n",
      "[151/569] âœ… thompkl01 â€” written\n",
      "[152/569] âœ… washipj01 â€” written\n",
      "[153/569] âœ… willibr03 â€” written\n",
      "[154/569] âœ… alexatr01 â€” written\n",
      "[155/569] âœ… braunch01 â€” written\n",
      "[156/569] âœ… cancavl01 â€” written\n",
      "[157/569] âœ… gordoaa01 â€” written\n",
      "[158/569] âœ… hallpj01 â€” written\n",
      "[159/569] âœ… jokicni01 â€” written\n",
      "[160/569] âœ… jonessp01 â€” written\n",
      "[161/569] âœ… jordade01 â€” written\n",
      "[162/569] âœ… murraja01 â€” written\n",
      "[163/569] âœ… nnajize01 â€” written\n",
      "[164/569] âœ… pickeja02 â€” written\n",
      "[165/569] âœ… portemi01 â€” written\n",
      "[166/569] âœ… saricda01 â€” written\n",
      "[167/569] âœ… strawju01 â€” written\n",
      "[168/569] âœ… tysonhu01 â€” written\n",
      "[169/569] âœ… watsope01 â€” written\n",
      "[170/569] âœ… westbru01 â€” written\n",
      "[171/569] âœ… beaslma01 â€” written\n",
      "[172/569] âœ… cunnica01 â€” written\n",
      "[173/569] âœ… durenja01 â€” written\n",
      "[174/569] âœ… fontesi01 â€” written\n",
      "[175/569] âœ… hardati02 â€” written\n",
      "[176/569] âœ… harpero02 â€” written\n",
      "[177/569] âœ… harrito02 â€” written\n",
      "[178/569] âœ… hollaro01 â€” written\n",
      "[179/569] âœ… iveyja01 â€” written\n",
      "[180/569] âœ… jenkida01 â€” written\n",
      "[181/569] âœ… klintbo01 â€” written\n",
      "[182/569] âœ… reedpa01 â€” written\n",
      "[183/569] âœ… sassema01 â€” written\n",
      "[184/569] âœ… smithto05 â€” written\n",
      "[185/569] âœ… stewais01 â€” written\n",
      "[186/569] âœ… swideco01 â€” written\n",
      "[187/569] âœ… thompau01 â€” written\n",
      "[188/569] âœ… waterli01 â€” written\n",
      "[189/569] âœ… willial06 â€” written\n",
      "[190/569] âœ… anderky01 â€” written\n",
      "[191/569] âœ… butleji01 â€” written\n",
      "[192/569] âœ… colliyu01 â€” written\n",
      "[193/569] âœ… curryst01 â€” written\n",
      "[194/569] âœ… greendr01 â€” written\n",
      "[195/569] âœ… hieldbu01 â€” written\n",
      "[196/569] âœ… jackstr02 â€” written\n",
      "[197/569] âœ… keybr01 â€” written\n",
      "[198/569] âœ… knoxke01 â€” written\n",
      "[199/569] âœ… kuminjo01 â€” written\n",
      "[200/569] âœ… looneke01 â€” written\n",
      "[201/569] âœ… meltode01 â€” written\n",
      "[202/569] âœ… moodymo01 â€” written\n",
      "[203/569] âœ… paytoga02 â€” written\n",
      "[204/569] âœ… podzibr01 â€” written\n",
      "[205/569] âœ… postqu01 â€” written\n",
      "[206/569] âœ… roweja01 â€” written\n",
      "[207/569] âœ… santogu01 â€” written\n",
      "[208/569] âœ… spencpa01 â€” written\n",
      "[209/569] âœ… wiggian01 â€” written\n",
      "[210/569] âœ… adamsst01 â€” written\n",
      "[211/569] âœ… brookdi01 â€” written\n",
      "[212/569] âœ… nfalyda01 â€” written\n",
      "[213/569] âœ… easonta01 â€” written\n",
      "[214/569] âœ… greenja05 â€” written\n",
      "[215/569] âœ… greenje02 â€” written\n",
      "[216/569] âœ… holidaa01 â€” written\n",
      "[217/569] âœ… landajo01 â€” written\n",
      "[218/569] âœ… mcveija01 â€” written\n",
      "[219/569] âœ… sengual01 â€” written\n",
      "[220/569] âœ… sheppre01 â€” written\n",
      "[221/569] âœ… smithja05 â€” written\n",
      "[222/569] âœ… tateja01 â€” written\n",
      "[223/569] âœ… thompam01 â€” written\n",
      "[224/569] âœ… vanvlfr01 â€” written\n",
      "[225/569] âœ… whitmca01 â€” written\n",
      "[226/569] âœ… willije02 â€” written\n",
      "[227/569] âœ… bradlto01 â€” written\n",
      "[228/569] âœ… bryanth01 â€” written\n",
      "[229/569] âœ… dennira01 â€” written\n",
      "[230/569] âœ… freemen01 â€” written\n",
      "[231/569] âœ… furphjo01 â€” written\n",
      "[232/569] âœ… halibty01 â€” written\n",
      "[233/569] âœ… jacksis01 â€” written\n",
      "[234/569] âœ… jacksqu01 â€” written\n",
      "[235/569] âœ… johnsja01 â€” written\n",
      "[236/569] âœ… mathube01 â€” written\n",
      "[237/569] âœ… mccontj01 â€” written\n",
      "[238/569] âœ… nembhan01 â€” written\n",
      "[239/569] âœ… nesmiaa01 â€” written\n",
      "[240/569] âœ… newtotr01 â€” written\n",
      "[241/569] âœ… okafoja01 â€” written\n",
      "[242/569] âœ… sheppbe01 â€” written\n",
      "[243/569] âœ… siakapa01 â€” written\n",
      "[244/569] âœ… toppiob01 â€” written\n",
      "[245/569] âœ… turnemy01 â€” written\n",
      "[246/569] âœ… walkeja02 â€” written\n",
      "[247/569] âœ… wisemja01 â€” written\n",
      "[248/569] âœ… baldwpa01 â€” written\n",
      "[249/569] âœ… bambamo01 â€” written\n",
      "[250/569] âœ… batumni01 â€” written\n",
      "[251/569] âœ… beaucma01 â€” written\n",
      "[252/569] âœ… brownko01 â€” written\n",
      "[253/569] âœ… chrisca02 â€” written\n",
      "[254/569] âœ… coffeam01 â€” written\n",
      "[255/569] âœ… dunnkr01 â€” written\n",
      "[256/569] âœ… eubandr01 â€” written\n",
      "[257/569] âœ… flowetr01 â€” written\n",
      "[258/569] âœ… hardeja01 â€” written\n",
      "[259/569] âœ… hylanbo01 â€” written\n",
      "[260/569] âœ… jonesde02 â€” written\n",
      "[261/569] âœ… leonaka01 â€” written\n",
      "[262/569] âœ… millejo02 â€” written\n",
      "[263/569] âœ… millspa02 â€” written\n",
      "[264/569] âœ… porteke02 â€” written\n",
      "[265/569] âœ… powelno01 â€” written\n",
      "[266/569] âœ… zubaciv01 â€” written\n",
      "[267/569] âœ… goodwjo01 â€” written\n",
      "[268/569] âœ… hachiru01 â€” written\n",
      "[269/569] âœ… hayesja02 â€” written\n",
      "[270/569] âœ… hoodsja01 â€” written\n",
      "[271/569] âœ… jamesbr02 â€” written\n",
      "[272/569] âœ… jamesle01 â€” written\n",
      "[273/569] âœ… jemistr01 â€” written\n",
      "[274/569] âœ… knechda01 â€” written\n",
      "[275/569] âœ… kolokch01 â€” written\n",
      "[276/569] âœ… lenal01 â€” written\n",
      "[277/569] âœ… olivaqu01 â€” written\n",
      "[278/569] âœ… reaveau01 â€” written\n",
      "[279/569] âœ… reddica01 â€” written\n",
      "[280/569] âœ… armeltr01 â€” written\n",
      "[281/569] âœ… vandeja01 â€” written\n",
      "[282/569] âœ… vincega01 â€” written\n",
      "[283/569] âœ… aldamsa01 â€” written\n",
      "[284/569] âœ… baglema01 â€” written\n",
      "[285/569] âœ… banede01 â€” written\n",
      "[286/569] âœ… castlco01 â€” written\n",
      "[287/569] âœ… clarkbr01 â€” written\n",
      "[288/569] âœ… edeyza01 â€” written\n",
      "[289/569] âœ… huffja01 â€” written\n",
      "[290/569] âœ… jacksgg01 â€” written\n",
      "[291/569] âœ… jacksja02 â€” written\n",
      "[292/569] âœ… kawamyu01 â€” written\n",
      "[293/569] âœ… kennalu01 â€” written\n",
      "[294/569] âœ… konchjo01 â€” written\n",
      "[295/569] âœ… laravja01 â€” written\n",
      "[296/569] âœ… moranja01 â€” written\n",
      "[297/569] âœ… pippesc02 â€” written\n",
      "[298/569] âœ… pullizy01 â€” written\n",
      "[299/569] âœ… smartma01 â€” written\n",
      "[300/569] âœ… spencca01 â€” written\n",
      "[301/569] âœ… stevela01 â€” written\n",
      "[302/569] âœ… wellsja01 â€” written\n",
      "[303/569] âœ… willivi01 â€” written\n",
      "[304/569] âœ… adebaba01 â€” written\n",
      "[305/569] âœ… burksal01 â€” written\n",
      "[306/569] âœ… chrisjo01 â€” written\n",
      "[307/569] âœ… herroty01 â€” written\n",
      "[308/569] âœ… highsha01 â€” written\n",
      "[309/569] âœ… jaqueja01 â€” written\n",
      "[310/569] âœ… johnske10 â€” written\n",
      "[311/569] âœ… jovicni01 â€” written\n",
      "[312/569] âœ… larsspe01 â€” written\n",
      "[313/569] âœ… loveke01 â€” written\n",
      "[314/569] âœ… mitchda01 â€” written\n",
      "[315/569] âœ… richajo01 â€” written\n",
      "[316/569] âœ… robindu01 â€” written\n",
      "[317/569] âœ… roziete01 â€” written\n",
      "[318/569] âœ… smithdr01 â€” written\n",
      "[319/569] âœ… steveis01 â€” written\n",
      "[320/569] âœ… wareke01 â€” written\n",
      "[321/569] âœ… antetgi01 â€” written\n",
      "[322/569] âœ… bouyeja01 â€” written\n",
      "[323/569] âœ… connapa01 â€” written\n",
      "[324/569] âœ… greenaj01 â€” written\n",
      "[325/569] âœ… jacksan01 â€” written\n",
      "[326/569] âœ… johnsaj01 â€” written\n",
      "[327/569] âœ… kuzmaky01 â€” written\n",
      "[328/569] âœ… lillada01 â€” written\n",
      "[329/569] âœ… livinch01 â€” written\n",
      "[330/569] âœ… lopezbr01 â€” written\n",
      "[331/569] âœ… middlkh01 â€” written\n",
      "[332/569] âœ… nancepe01 â€” written\n",
      "[333/569] âœ… portibo01 â€” written\n",
      "[334/569] âœ… princta02 â€” written\n",
      "[335/569] âœ… robbili01 â€” written\n",
      "[336/569] âœ… rolliry01 â€” written\n",
      "[337/569] âœ… simsje01 â€” written\n",
      "[338/569] âœ… smithty02 â€” written\n",
      "[339/569] âœ… trentga02 â€” written\n",
      "[340/569] âœ… umudest01 â€” written\n",
      "[341/569] âœ… wrighde01 â€” written\n",
      "[342/569] âœ… alexani01 â€” written\n",
      "[343/569] âœ… clarkja02 â€” written\n",
      "[344/569] âœ… conlemi01 â€” written\n",
      "[345/569] âœ… dilliro01 â€” written\n",
      "[346/569] âœ… divindo01 â€” written\n",
      "[347/569] âœ… doziepj01 â€” written\n",
      "[348/569] âœ… edwaran01 â€” written\n",
      "[349/569] âœ… edwarje01 â€” written\n",
      "[350/569] âœ… garzalu01 â€” written\n",
      "[351/569] âœ… goberru01 â€” written\n",
      "[352/569] âœ… inglejo01 â€” written\n",
      "[353/569] âœ… mcdanja02 â€” written\n",
      "[354/569] âœ… millele01 â€” written\n",
      "[355/569] âœ… minotjo01 â€” written\n",
      "[356/569] âœ… nixda01 â€” written\n",
      "[357/569] âœ… randlju01 â€” written\n",
      "[358/569] âœ… reidna01 â€” written\n",
      "[359/569] âœ… shannte01 â€” written\n",
      "[360/569] âœ… alvarjo01 â€” written\n",
      "[361/569] âœ… bostobr01 â€” written\n",
      "[362/569] âœ… brookke02 â€” written\n",
      "[363/569] âœ… brownbr01 â€” written\n",
      "[364/569] âœ… cainja01 â€” written\n",
      "[365/569] âœ… hawkijo01 â€” written\n",
      "[366/569] âœ… ingrabr01 â€” written\n",
      "[367/569] âœ… joneshe01 â€” written\n",
      "[368/569] âœ… matkoka01 â€” written\n",
      "[369/569] âœ… mccolcj01 â€” written\n",
      "[370/569] âœ… missiyv01 â€” written\n",
      "[371/569] âœ… murphtr02 â€” written\n",
      "[372/569] âœ… murrade01 â€” written\n",
      "[373/569] âœ… nowelja01 â€” written\n",
      "[374/569] âœ… olynyke01 â€” written\n",
      "[375/569] âœ… quinole01 â€” written\n",
      "[376/569] âœ… reevean01 â€” written\n",
      "[377/569] âœ… robinje02 â€” written\n",
      "[378/569] âœ… theisda01 â€” written\n",
      "[379/569] âœ… willizi01 â€” written\n",
      "[380/569] âœ… achiupr01 â€” written\n",
      "[381/569] âœ… anunoog01 â€” written\n",
      "[382/569] âœ… bridgmi01 â€” written\n",
      "[383/569] âœ… brunsja01 â€” written\n",
      "[384/569] âœ… dadiepa01 â€” written\n",
      "[385/569] âœ… hartjo01 â€” written\n",
      "[386/569] âœ… hukpoar01 â€” written\n",
      "[387/569] âœ… kolekty01 â€” written\n",
      "[388/569] âœ… mcbrimi01 â€” written\n",
      "[389/569] âœ… mcculke01 â€” written\n",
      "[390/569] âœ… payneca01 â€” written\n",
      "[391/569] âœ… robinmi01 â€” written\n",
      "[392/569] âœ… ryanma01 â€” written\n",
      "[393/569] âœ… shamela01 â€” written\n",
      "[394/569] âœ… townska01 â€” written\n",
      "[395/569] âœ… tuckepj01 â€” written\n",
      "[396/569] âœ… watsoan02 â€” written\n",
      "[397/569] âœ… carlsbr01 â€” written\n",
      "[398/569] âœ… carusal01 â€” written\n",
      "[399/569] âœ… diengou01 â€” written\n",
      "[400/569] âœ… dortlu01 â€” written\n",
      "[401/569] âœ… ducasal01 â€” written\n",
      "[402/569] âœ… flaglad01 â€” written\n",
      "[403/569] âœ… gilgesh01 â€” written\n",
      "[404/569] âœ… harteis01 â€” written\n",
      "[405/569] âœ… holmgch01 â€” written\n",
      "[406/569] âœ… joeis01 â€” written\n",
      "[407/569] âœ… jonesdi01 â€” written\n",
      "[408/569] âœ… leonsma01 â€” written\n",
      "[409/569] âœ… mitchaj01 â€” written\n",
      "[410/569] âœ… reeseal01 â€” written\n",
      "[411/569] âœ… wallaca01 â€” written\n",
      "[412/569] âœ… wiggiaa01 â€” written\n",
      "[413/569] âœ… willija06 â€” written\n",
      "[414/569] âœ… willija07 â€” written\n",
      "[415/569] âœ… willike04 â€” written\n",
      "[416/569] âœ… anthoco01 â€” written\n",
      "[417/569] âœ… banchpa01 â€” written\n",
      "[418/569] âœ… bitadgo01 â€” written\n",
      "[419/569] âœ… blackan01 â€” written\n",
      "[420/569] âœ… caldwke01 â€” written\n",
      "[421/569] âœ… cartewe01 â€” written\n",
      "[422/569] âœ… dasiltr01 â€” written\n",
      "[423/569] âœ… harriga01 â€” written\n",
      "[424/569] âœ… houstca01 â€” written\n",
      "[425/569] âœ… howarje01 â€” written\n",
      "[426/569] âœ… isaacjo01 â€” written\n",
      "[427/569] âœ… josepco01 â€” written\n",
      "[428/569] âœ… mccluma01 â€” written\n",
      "[429/569] âœ… queentr01 â€” written\n",
      "[430/569] âœ… suggsja01 â€” written\n",
      "[431/569] âœ… wagnefr01 â€” written\n",
      "[432/569] âœ… wagnemo01 â€” written\n",
      "[433/569] âœ… baglema02 â€” written\n",
      "[434/569] âœ… bonaad01 â€” written\n",
      "[435/569] âœ… brissos01 â€” written\n",
      "[436/569] âœ… butleja02 â€” written\n",
      "[437/569] âœ… councri01 â€” written\n",
      "[438/569] âœ… dowtije01 â€” written\n",
      "[439/569] âœ… drumman01 â€” written\n",
      "[440/569] âœ… edwarju01 â€” written\n",
      "[441/569] âœ… embiijo01 â€” written\n",
      "[442/569] âœ… georgpa01 â€” written\n",
      "[443/569] âœ… gordoer01 â€” written\n",
      "[444/569] âœ… jacksre01 â€” written\n",
      "[445/569] âœ… lowryky01 â€” written\n",
      "[446/569] âœ… martike04 â€” written\n",
      "[447/569] âœ… maxeyty01 â€” written\n",
      "[448/569] âœ… mccaija01 â€” written\n",
      "[449/569] âœ… mobleis01 â€” written\n",
      "[450/569] âœ… oubreke01 â€” written\n",
      "[451/569] âœ… walkelo01 â€” written\n",
      "[452/569] âœ… wheelph02 â€” written\n",
      "[453/569] âœ… yabusgu01 â€” written\n",
      "[454/569] âœ… allengr01 â€” written\n",
      "[455/569] âœ… bealbr01 â€” written\n",
      "[456/569] âœ… bolbo01 â€” written\n",
      "[457/569] âœ… bookede01 â€” written\n",
      "[458/569] âœ… bridgja01 â€” written\n",
      "[459/569] âœ… dunnry01 â€” written\n",
      "[460/569] âœ… duranke01 â€” written\n",
      "[461/569] âœ… gilleco01 â€” written\n",
      "[462/569] âœ… ighodos01 â€” written\n",
      "[463/569] âœ… jonesty01 â€” written\n",
      "[464/569] âœ… leeda03 â€” written\n",
      "[465/569] âœ… morrimo01 â€” written\n",
      "[466/569] âœ… onealro01 â€” written\n",
      "[467/569] âœ… plumlma01 â€” written\n",
      "[468/569] âœ… washity02 â€” written\n",
      "[469/569] âœ… avdijde01 â€” written\n",
      "[470/569] âœ… aytonde01 â€” written\n",
      "[471/569] âœ… bantoda01 â€” written\n",
      "[472/569] âœ… camarto01 â€” written\n",
      "[473/569] âœ… cissosi01 â€” written\n",
      "[474/569] âœ… clingdo01 â€” written\n",
      "[475/569] âœ… grantje01 â€” written\n",
      "[476/569] âœ… hendesc01 â€” written\n",
      "[477/569] âœ… mcgowbr01 â€” written\n",
      "[478/569] âœ… minayju01 â€” written\n",
      "[479/569] âœ… mooreta02 â€” written\n",
      "[480/569] âœ… murrakr01 â€” written\n",
      "[481/569] âœ… reathdu01 â€” written\n",
      "[482/569] âœ… ruperra01 â€” written\n",
      "[483/569] âœ… sharpsh01 â€” written\n",
      "[484/569] âœ… simonan01 â€” written\n",
      "[485/569] âœ… thybuma01 â€” written\n",
      "[486/569] âœ… walkeja01 â€” written\n",
      "[487/569] âœ… williro04 â€” written\n",
      "[488/569] âœ… cartede02 â€” written\n",
      "[489/569] âœ… crawfis01 â€” written\n",
      "[490/569] âœ… crowdja01 â€” written\n",
      "[491/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/d/daviste02.html: Failed to fetch https://www.basketball-reference.com/players/d/daviste02.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/d/daviste02.html\n",
      "[492/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/d/derozde01.html: Failed to fetch https://www.basketball-reference.com/players/d/derozde01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/d/derozde01.html\n",
      "[493/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/e/elliske01.html: Failed to fetch https://www.basketball-reference.com/players/e/elliske01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/e/elliske01.html\n",
      "[494/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/f/foxde01.html: Failed to fetch https://www.basketball-reference.com/players/f/foxde01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/f/foxde01.html\n",
      "[495/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/f/fultzma01.html: Failed to fetch https://www.basketball-reference.com/players/f/fultzma01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/f/fultzma01.html\n",
      "[496/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/j/jonesco02.html: Failed to fetch https://www.basketball-reference.com/players/j/jonesco02.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/j/jonesco02.html\n",
      "[497/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/j/jonesis01.html: Failed to fetch https://www.basketball-reference.com/players/j/jonesis01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/j/jonesis01.html\n",
      "[498/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/j/jonesma05.html: Failed to fetch https://www.basketball-reference.com/players/j/jonesma05.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/j/jonesma05.html\n",
      "[499/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/l/labissk01.html: Failed to fetch https://www.basketball-reference.com/players/l/labissk01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/l/labissk01.html\n",
      "[500/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/l/lylestr01.html: Failed to fetch https://www.basketball-reference.com/players/l/lylestr01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/l/lylestr01.html\n",
      "[501/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/m/mcderdo01.html: Failed to fetch https://www.basketball-reference.com/players/m/mcderdo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mcderdo01.html\n",
      "[502/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/m/mclaujo01.html: Failed to fetch https://www.basketball-reference.com/players/m/mclaujo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mclaujo01.html\n",
      "[503/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/m/monkma01.html: Failed to fetch https://www.basketball-reference.com/players/m/monkma01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/monkma01.html\n",
      "[504/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/m/murrake02.html: Failed to fetch https://www.basketball-reference.com/players/m/murrake02.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/murrake02.html\n",
      "[505/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/r/robinor01.html: Failed to fetch https://www.basketball-reference.com/players/r/robinor01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/r/robinor01.html\n",
      "[506/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/s/sabondo01.html: Failed to fetch https://www.basketball-reference.com/players/s/sabondo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sabondo01.html\n",
      "[507/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/t/taylote01.html: Failed to fetch https://www.basketball-reference.com/players/t/taylote01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/t/taylote01.html\n",
      "[508/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/v/valanjo01.html: Failed to fetch https://www.basketball-reference.com/players/v/valanjo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/v/valanjo01.html\n",
      "[509/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/b/barneha02.html: Failed to fetch https://www.basketball-reference.com/players/b/barneha02.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/barneha02.html\n",
      "[510/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/b/bassech01.html: Failed to fetch https://www.basketball-reference.com/players/b/bassech01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/bassech01.html\n",
      "[511/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/b/biyombi01.html: Failed to fetch https://www.basketball-reference.com/players/b/biyombi01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/biyombi01.html\n",
      "[512/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/b/branhma01.html: Failed to fetch https://www.basketball-reference.com/players/b/branhma01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/branhma01.html\n",
      "[513/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/c/castlst01.html: Failed to fetch https://www.basketball-reference.com/players/c/castlst01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/castlst01.html\n",
      "[514/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/c/champju02.html: Failed to fetch https://www.basketball-reference.com/players/c/champju02.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/champju02.html\n",
      "[515/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/d/dukeda01.html: Failed to fetch https://www.basketball-reference.com/players/d/dukeda01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/d/dukeda01.html\n",
      "[516/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/i/ingraha01.html: Failed to fetch https://www.basketball-reference.com/players/i/ingraha01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/i/ingraha01.html\n",
      "[517/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/j/johnske04.html: Failed to fetch https://www.basketball-reference.com/players/j/johnske04.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/j/johnske04.html\n",
      "[518/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/m/mamuksa01.html: Failed to fetch https://www.basketball-reference.com/players/m/mamuksa01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mamuksa01.html\n",
      "[519/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/m/minixri01.html: Failed to fetch https://www.basketball-reference.com/players/m/minixri01.html after 5 attempts. Last error: HTTPSConnectionPool(host='api.scraperapi.com', port=443): Read timed out. (read timeout=45)\n",
      "[520/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/p/paulch01.html: Failed to fetch https://www.basketball-reference.com/players/p/paulch01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/p/paulch01.html\n",
      "[521/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/s/sochaje01.html: Failed to fetch https://www.basketball-reference.com/players/s/sochaje01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sochaje01.html\n",
      "[522/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/v/vassede01.html: Failed to fetch https://www.basketball-reference.com/players/v/vassede01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/v/vassede01.html\n",
      "[523/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/w/wembavi01.html: Failed to fetch https://www.basketball-reference.com/players/w/wembavi01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/w/wembavi01.html\n",
      "[524/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/w/weslebl01.html: Failed to fetch https://www.basketball-reference.com/players/w/weslebl01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/w/weslebl01.html\n",
      "[525/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/a/agbajoc01.html: Failed to fetch https://www.basketball-reference.com/players/a/agbajoc01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/a/agbajoc01.html\n",
      "[526/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/b/barnesc01.html: Failed to fetch https://www.basketball-reference.com/players/b/barnesc01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/barnesc01.html\n",
      "[527/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/b/barrerj01.html: Failed to fetch https://www.basketball-reference.com/players/b/barrerj01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/barrerj01.html\n",
      "[528/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/b/battlja01.html: Failed to fetch https://www.basketball-reference.com/players/b/battlja01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/battlja01.html\n",
      "[529/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/b/bouchch01.html: Failed to fetch https://www.basketball-reference.com/players/b/bouchch01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/bouchch01.html\n",
      "[530/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/c/cartodj01.html: Failed to fetch https://www.basketball-reference.com/players/c/cartodj01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/cartodj01.html\n",
      "[531/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/c/chomcul01.html: Failed to fetch https://www.basketball-reference.com/players/c/chomcul01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/chomcul01.html\n",
      "[532/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/d/dickgr01.html: Failed to fetch https://www.basketball-reference.com/players/d/dickgr01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/d/dickgr01.html\n",
      "[533/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/f/fernabr01.html: Failed to fetch https://www.basketball-reference.com/players/f/fernabr01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/f/fernabr01.html\n",
      "[534/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/l/lawsoaj01.html: Failed to fetch https://www.basketball-reference.com/players/l/lawsoaj01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/l/lawsoaj01.html\n",
      "[535/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/m/mogbojo01.html: Failed to fetch https://www.basketball-reference.com/players/m/mogbojo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mogbojo01.html\n",
      "[536/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/p/poeltja01.html: Failed to fetch https://www.basketball-reference.com/players/p/poeltja01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/p/poeltja01.html\n",
      "[537/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/q/quickim01.html: Failed to fetch https://www.basketball-reference.com/players/q/quickim01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/q/quickim01.html\n",
      "[538/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/s/sheadja01.html: Failed to fetch https://www.basketball-reference.com/players/s/sheadja01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sheadja01.html\n",
      "[539/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/t/templga01.html: Failed to fetch https://www.basketball-reference.com/players/t/templga01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/t/templga01.html\n",
      "[540/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/w/walteja01.html: Failed to fetch https://www.basketball-reference.com/players/w/walteja01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/w/walteja01.html\n",
      "[541/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/c/clarkjo01.html: Failed to fetch https://www.basketball-reference.com/players/c/clarkjo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/clarkjo01.html\n",
      "[542/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/c/colliis01.html: Failed to fetch https://www.basketball-reference.com/players/c/colliis01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/colliis01.html\n",
      "[543/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/c/collijo01.html: Failed to fetch https://www.basketball-reference.com/players/c/collijo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/collijo01.html\n",
      "[544/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/f/filipky01.html: Failed to fetch https://www.basketball-reference.com/players/f/filipky01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/f/filipky01.html\n",
      "[545/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/g/georgke01.html: Failed to fetch https://www.basketball-reference.com/players/g/georgke01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/g/georgke01.html\n",
      "[546/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/h/harklej01.html: Failed to fetch https://www.basketball-reference.com/players/h/harklej01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/h/harklej01.html\n",
      "[547/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/h/hendrta01.html: Failed to fetch https://www.basketball-reference.com/players/h/hendrta01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/h/hendrta01.html\n",
      "[548/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/j/juzanjo01.html: Failed to fetch https://www.basketball-reference.com/players/j/juzanjo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/j/juzanjo01.html\n",
      "[549/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/k/kesslwa01.html: Failed to fetch https://www.basketball-reference.com/players/k/kesslwa01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/k/kesslwa01.html\n",
      "[550/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/m/markkla01.html: Failed to fetch https://www.basketball-reference.com/players/m/markkla01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/markkla01.html\n",
      "[551/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/m/mykhasv01.html: Failed to fetch https://www.basketball-reference.com/players/m/mykhasv01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mykhasv01.html\n",
      "[552/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/p/pottemi01.html: Failed to fetch https://www.basketball-reference.com/players/p/pottemi01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/p/pottemi01.html\n",
      "[553/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/s/sensabr01.html: Failed to fetch https://www.basketball-reference.com/players/s/sensabr01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sensabr01.html\n",
      "[554/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/s/sextoco01.html: Failed to fetch https://www.basketball-reference.com/players/s/sextoco01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sextoco01.html\n",
      "[555/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/t/tshieos01.html: Failed to fetch https://www.basketball-reference.com/players/t/tshieos01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/t/tshieos01.html\n",
      "[556/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/w/willico04.html: Failed to fetch https://www.basketball-reference.com/players/w/willico04.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/w/willico04.html\n",
      "[557/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/b/brogdma01.html: Failed to fetch https://www.basketball-reference.com/players/b/brogdma01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/brogdma01.html\n",
      "[558/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/c/carrica01.html: Failed to fetch https://www.basketball-reference.com/players/c/carrica01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/carrica01.html\n",
      "[559/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/c/champju01.html: Failed to fetch https://www.basketball-reference.com/players/c/champju01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/champju01.html\n",
      "[560/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/c/coulibi01.html: Failed to fetch https://www.basketball-reference.com/players/c/coulibi01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/coulibi01.html\n",
      "[561/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/d/davisjo06.html: Failed to fetch https://www.basketball-reference.com/players/d/davisjo06.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/d/davisjo06.html\n",
      "[562/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/g/georgky01.html: Failed to fetch https://www.basketball-reference.com/players/g/georgky01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/g/georgky01.html\n",
      "[563/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/g/gillan01.html: Failed to fetch https://www.basketball-reference.com/players/g/gillan01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/g/gillan01.html\n",
      "[564/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/h/holmeri01.html: Failed to fetch https://www.basketball-reference.com/players/h/holmeri01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/h/holmeri01.html\n",
      "[565/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/k/kispeco01.html: Failed to fetch https://www.basketball-reference.com/players/k/kispeco01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/k/kispeco01.html\n",
      "[566/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/m/mcdanja01.html: Failed to fetch https://www.basketball-reference.com/players/m/mcdanja01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mcdanja01.html\n",
      "[567/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/p/poolejo01.html: Failed to fetch https://www.basketball-reference.com/players/p/poolejo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/p/poolejo01.html\n",
      "[568/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/s/sarral01.html: Failed to fetch https://www.basketball-reference.com/players/s/sarral01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sarral01.html\n",
      "[569/569] ðŸ’¥ Error for https://www.basketball-reference.com/players/v/vukcetr01.html: Failed to fetch https://www.basketball-reference.com/players/v/vukcetr01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/v/vukcetr01.html\n",
      "ðŸ“ Finished. CSV written to output_data/all_nba_players_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Networking + DOM helpers\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def fetch_html_via_scraperapi(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    retries: int = 5,\n",
    "    timeout: int = 45,\n",
    "    jitter_range = (0.2, 0.7),\n",
    "    base: str = \"https://api.scraperapi.com\",\n",
    "    user_agent: str = \"Mozilla/5.0 (compatible; BBRBot/1.0)\"\n",
    ") -> str:\n",
    "    key = \"aaa492ea5514911b40ac2e7679e21da7\"  # hardcoded per your request\n",
    "    if not key:\n",
    "        raise RuntimeError(\"SCRAPERAPI_KEY not set. export SCRAPERAPI_KEY='your_key' or pass api_key.\")\n",
    "    params = {\"api_key\": key, \"url\": url}\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            r = requests.get(base, params=params, headers=headers, timeout=timeout)\n",
    "            if r.status_code == 200 and r.text:\n",
    "                return r.text\n",
    "            last_err = RuntimeError(f\"HTTP {r.status_code} for {url}\")\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e\n",
    "        time.sleep(min(1.5 ** attempt, 12) + random.uniform(*jitter_range))\n",
    "    raise RuntimeError(f\"Failed to fetch {url} after {retries} attempts. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "def _build_dom(html_text: str) -> html.HtmlElement:\n",
    "    \"\"\"\n",
    "    Build an lxml DOM and also append any tables that may be hidden inside HTML comments.\n",
    "    Basketball-Reference sometimes wraps tables in comments (e.g., per_game, per_minute, per_poss, advanced, contracts_*).\n",
    "    \"\"\"\n",
    "    # Use substrings (not exact ids) so contracts_* (contracts_lac, contracts_pho...) get included.\n",
    "    TABLE_ID_SUBSTRINGS = (\n",
    "        \"per_game_stats\",\n",
    "        \"per_minute_stats\",\n",
    "        \"per_poss\",\n",
    "        \"advanced\",\n",
    "        \"all_salaries\",\n",
    "        \"contracts_\",   # <-- match any contracts_* table\n",
    "    )\n",
    "\n",
    "    dom = html.fromstring(html_text)\n",
    "\n",
    "    for c in dom.xpath('//comment()'):\n",
    "        c_text = c.text or \"\"\n",
    "        if \"<table\" not in c_text:\n",
    "            continue\n",
    "        # Only unhide if the comment contains one of our desired table id substrings\n",
    "        if any(sub in c_text for sub in TABLE_ID_SUBSTRINGS):\n",
    "            try:\n",
    "                sub = html.fromstring(c_text)\n",
    "                for t in sub.xpath(\".//table\"):\n",
    "                    tid = t.get(\"id\") or \"\"\n",
    "                    # Append if the id contains any of our substrings\n",
    "                    if any(sub in tid for sub in TABLE_ID_SUBSTRINGS):\n",
    "                        dom.append(t)\n",
    "            except Exception:\n",
    "                # Ignore parsing errors in odd comment blocks\n",
    "                pass\n",
    "\n",
    "    return dom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Personal info (player_id logic inlined here)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def players_personal_info(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Fetches HTML if needed and parses personal info.\n",
    "    player_id is derived inline from the URL.\n",
    "    \"\"\"\n",
    "    # inline player_id extraction\n",
    "    m = re.search(r\"/([^/]+)\\.html?$\", url)\n",
    "    player_id = m.group(1).lower() if m else None\n",
    "\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    # player_name\n",
    "    player_name = ''.join(dom.xpath(\"//div[@id='meta']//h1//span/text()\")).strip()\n",
    "    if not player_name:\n",
    "        og = dom.xpath(\"//meta[@property='og:title']/@content\")\n",
    "        if og:\n",
    "            player_name = og[0].split(\" Stats\", 1)[0].strip()\n",
    "\n",
    "    # team\n",
    "    team = ''.join(dom.xpath(\"//div[@id='meta']//p[strong[normalize-space()='Team']]/a/text()\")).strip()\n",
    "    if not team:\n",
    "        p_text = dom.xpath(\"normalize-space(//div[@id='meta']//p[strong[normalize-space()='Team']])\")\n",
    "        if p_text and \":\" in p_text:\n",
    "            team = p_text.split(\":\", 1)[1].strip()\n",
    "\n",
    "    # birth_day\n",
    "    birth_day = ''.join(dom.xpath(\"//strong[normalize-space()='Born:']/following-sibling::span[@id='necro-birth']/@data-birth\")).strip()\n",
    "    if not birth_day:\n",
    "        birth_day = ''.join(dom.xpath(\"//strong[contains(.,'Born')]/following-sibling::span[@id='necro-birth']/@data-birth\")).strip()\n",
    "\n",
    "    # years_experience\n",
    "    exp_line = dom.xpath(\"string(//div[@id='meta']//p[strong[normalize-space()='Experience:']])\")\n",
    "    years_experience = None\n",
    "    if exp_line:\n",
    "        m_exp = re.search(r\"Experience:\\s*(\\d+)\", exp_line, flags=re.IGNORECASE)\n",
    "        if m_exp:\n",
    "            try:\n",
    "                years_experience = int(m_exp.group(1))\n",
    "            except ValueError:\n",
    "                years_experience = None\n",
    "\n",
    "    return {\n",
    "        \"player_id\": player_id,\n",
    "        \"player_name\": player_name or None,\n",
    "        \"team\": team or None,\n",
    "        \"birth_day\": birth_day or None,\n",
    "        \"years_experience\": years_experience\n",
    "    }\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Per Game\n",
    "#  - rename year_id -> season\n",
    "#  - no 'per_g' on specific fields; rename mapping below\n",
    "#  - keep 'age' as 'age'\n",
    "#  - all other fields get '_per_g' appended if not present\n",
    "#  - OUTPUT ORDER: strictly leftâ†’right by table header\n",
    "#  - NEW: was_traded, teams_count, teams_played (abbr list, comma-separated)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def per_game_stats(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    prefer_tot: bool = True,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    table_nodes = dom.xpath(\"//table[@id='per_game_stats']\")\n",
    "    if not table_nodes:\n",
    "        return None\n",
    "    table = table_nodes[0]\n",
    "\n",
    "    # Build header order (leftâ†’right)\n",
    "    header_stats: List[str] = []\n",
    "    for th in table.xpath(\".//thead//tr[1]/th\"):\n",
    "        ds = th.get(\"data-stat\")\n",
    "        if ds:\n",
    "            header_stats.append(ds)\n",
    "\n",
    "    # ---------- current season (tbody) ----------\n",
    "    rows = table.xpath(\".//tbody/tr[th[@data-stat='year_id']]\")\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    seasons: List[tuple[int, Any]] = []\n",
    "    for tr in rows:\n",
    "        th = tr.xpath(\"./th[@data-stat='year_id']\")[0]\n",
    "        csk = th.get(\"csk\")\n",
    "        try:\n",
    "            csk_int = int(csk)\n",
    "        except (TypeError, ValueError):\n",
    "            txt = th.xpath(\"normalize-space(string(.))\")\n",
    "            m = re.search(r\"(\\d{4})(?:-\\d{2})?$\", txt or \"\")\n",
    "            csk_int = int(m.group(1)) if m else 0\n",
    "        seasons.append((csk_int, tr))\n",
    "\n",
    "    max_csk = max(c for c, _ in seasons)\n",
    "    latest_rows = [tr for (c, tr) in seasons if c == max_csk]\n",
    "\n",
    "    # Gather all team abbreviations for that season (exclude TOT), preserve order, de-dup\n",
    "    teams_abbrs: List[str] = []\n",
    "    seen_team = set()\n",
    "    for tr in latest_rows:\n",
    "        abbr = ''.join(tr.xpath(\"./td[@data-stat='team_name_abbr']//text()\")).strip()\n",
    "        if not abbr or abbr.upper() == \"TOT\":\n",
    "            continue\n",
    "        if abbr not in seen_team:\n",
    "            seen_team.add(abbr)\n",
    "            teams_abbrs.append(abbr)\n",
    "\n",
    "    was_traded = \"Y\" if len(teams_abbrs) > 1 else \"N\"\n",
    "    teams_count = 1 if len(teams_abbrs) == 1 else len(teams_abbrs) - 1\n",
    "    teams_played = \",\".join(teams_abbrs)\n",
    "\n",
    "    # Prefer TOT if traded for the actual stat line chosen\n",
    "    if len(latest_rows) == 1 or not prefer_tot:\n",
    "        chosen = latest_rows[0]\n",
    "    else:\n",
    "        chosen = None\n",
    "        for tr in latest_rows:\n",
    "            team_txt = ''.join(tr.xpath(\"./td[@data-stat='team_name_abbr']//text()\")).strip()\n",
    "            if team_txt.upper() == \"TOT\":\n",
    "                chosen = tr\n",
    "                break\n",
    "        if chosen is None:\n",
    "            chosen = latest_rows[0]\n",
    "\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # season comes from th[data-stat=year_id]\n",
    "    th = chosen.xpath(\"./th[@data-stat='year_id']\")[0]\n",
    "    out[\"season\"] = th.xpath(\"normalize-space(string(.))\")  # e.g., \"2024-25\"\n",
    "\n",
    "    # Lookup of row cells by data-stat\n",
    "    td_by_stat = {td.get(\"data-stat\"): td for td in chosen.xpath(\"./td[@data-stat]\")}\n",
    "\n",
    "    # Fields that should NOT get \"_per_g\" and/or need renaming\n",
    "    no_per_g_and_rename = {\n",
    "        \"team_name_abbr\": \"team_id\",\n",
    "        \"comp_name_abbr\": \"lg_id\",\n",
    "        \"pos\": \"pos\",\n",
    "        \"games\": \"g\",\n",
    "        \"games_started\": \"gs\",\n",
    "        \"awards\": \"awards\",\n",
    "    }\n",
    "\n",
    "    # Iterate columns in the headerâ€™s leftâ†’right order for current season\n",
    "    for stat in header_stats:\n",
    "        if stat == \"year_id\":\n",
    "            continue  # already 'season'\n",
    "\n",
    "        td = td_by_stat.get(stat)\n",
    "        if td is None:\n",
    "            continue\n",
    "        val = td.xpath(\"normalize-space(string(.))\")\n",
    "\n",
    "        if stat == \"age\":\n",
    "            out[\"age\"] = val\n",
    "            continue\n",
    "\n",
    "        if stat in no_per_g_and_rename:\n",
    "            out[no_per_g_and_rename[stat]] = val\n",
    "        else:\n",
    "            key = stat if stat.endswith(\"_per_g\") else f\"{stat}_per_g\"\n",
    "            out[key] = val\n",
    "\n",
    "    # Append trade metadata fields\n",
    "    out[\"was_traded\"] = was_traded\n",
    "    out[\"teams_count\"] = teams_count\n",
    "    out[\"teams_played\"] = teams_played\n",
    "\n",
    "    # ---------- career row (tfoot) ----------\n",
    "    # We want the overall career (e.g., \"3 Yrs\"), not franchise-specific summaries.\n",
    "    tfoot = table.xpath(\"./tfoot\")\n",
    "    if tfoot:\n",
    "        tf = tfoot[0]\n",
    "        candidates = tf.xpath(\".//tr[th[@data-stat='year_id'] and .//td[@data-stat]]\")\n",
    "        career_tr = None\n",
    "        if candidates:\n",
    "            # Choose the row with the largest number of seasons (\"X Yrs\") or widest colspan.\n",
    "            best = None\n",
    "            best_score = -1\n",
    "            for tr in candidates:\n",
    "                th = tr.xpath(\"./th[@data-stat='year_id']\")[0]\n",
    "                # Score #1: parse \"X Yrs\" text\n",
    "                txt = th.xpath(\"normalize-space(string(.))\")\n",
    "                m_yrs = re.search(r\"(\\d+)\\s+Yrs\", txt)\n",
    "                yrs_val = int(m_yrs.group(1)) if m_yrs else 0\n",
    "                # Score #2: colspan fallback\n",
    "                colspan = th.get(\"colspan\")\n",
    "                try:\n",
    "                    colspan_val = int(colspan)\n",
    "                except (TypeError, ValueError):\n",
    "                    colspan_val = 0\n",
    "                score = max(yrs_val, colspan_val)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best = tr\n",
    "            career_tr = best\n",
    "\n",
    "        if career_tr is not None:\n",
    "            # Build a lookup of TDs by data-stat for the chosen career row\n",
    "            c_td_by_stat = {td.get(\"data-stat\"): td for td in career_tr.xpath(\"./td[@data-stat]\")}\n",
    "\n",
    "            # Exclude fields that don't make sense on career summary\n",
    "            career_exclude = {\n",
    "                \"year_id\", \"team_name_abbr\", \"comp_name_abbr\", \"pos\", \"awards\"\n",
    "            }\n",
    "\n",
    "            for stat in header_stats:\n",
    "                if stat in career_exclude:\n",
    "                    continue\n",
    "                td = c_td_by_stat.get(stat)\n",
    "                if td is None:\n",
    "                    continue\n",
    "                val = td.xpath(\"normalize-space(string(.))\")\n",
    "\n",
    "                # Prefix every kept stat with 'career_'\n",
    "                out[f\"career_{stat}\"] = val\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "#Parse the 'Per 36 Minutes' table (id='per_minute_stats') and return the latest season row.\n",
    " # - Always picks the current season (max csk), preferring TOT if traded (when prefer_tot=True).\n",
    " # - Excludes fields: season, age, team_id, lg_id, pos, g, gs, awards, and trade metadata.\n",
    " # - For any data-stat that doesn't already include 'per_minute_36', append '_per_minute_36' to the output key.\n",
    " # - Preserves header leftâ†’right order.\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def per_36_min_stats(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    prefer_tot: bool = True,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    # â”€â”€ fetch & DOM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    tables = dom.xpath(\"//table[@id='per_minute_stats']\")\n",
    "    if not tables:\n",
    "        return None\n",
    "    table = tables[0]\n",
    "\n",
    "    # â”€â”€ header (leftâ†’right) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    header_stats: List[str] = []\n",
    "    ths = table.xpath(\".//thead//tr[1]/th\")\n",
    "    for th in ths:\n",
    "        ds = th.get(\"data-stat\")\n",
    "        if ds:\n",
    "            header_stats.append(ds)\n",
    "\n",
    "    # â”€â”€ body rows with a season cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    rows = table.xpath(\".//tbody/tr[th[@data-stat='year_id']]\")\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    # Build (csk_int, tr)\n",
    "    seasons: List[tuple[int, Any]] = []\n",
    "    for tr in rows:\n",
    "        th = tr.xpath(\"./th[@data-stat='year_id']\")\n",
    "        if not th:\n",
    "            continue\n",
    "        csk = th[0].get(\"csk\")\n",
    "        csk_int = 0\n",
    "        if csk is not None:\n",
    "            try:\n",
    "                csk_int = int(csk)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if csk_int == 0:\n",
    "            txt = th[0].xpath(\"normalize-space(string(.))\")\n",
    "            m = re.search(r\"(\\d{4})(?:-\\d{2})?$\", txt or \"\")\n",
    "            if m:\n",
    "                try:\n",
    "                    csk_int = int(m.group(1))\n",
    "                except ValueError:\n",
    "                    csk_int = 0\n",
    "        seasons.append((csk_int, tr))\n",
    "\n",
    "    if not seasons:\n",
    "        return None\n",
    "\n",
    "    max_csk = max(c for c, _ in seasons)\n",
    "    latest_rows = [tr for (c, tr) in seasons if c == max_csk]\n",
    "\n",
    "    # â”€â”€ config for selection/aggregation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    aggregate_markers = {\"TOT\", \"2TM\", \"3TM\", \"4TM\"}\n",
    "    exclude_stats = {\n",
    "        \"year_id\", \"age\", \"team_name_abbr\", \"comp_name_abbr\",\n",
    "        \"pos\", \"awards\"\n",
    "    }\n",
    "\n",
    "    # â”€â”€ pick chosen row if an aggregate one exists (TOT/2TM/3TM/4TM) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    chosen = None\n",
    "    if prefer_tot and len(latest_rows) > 1:\n",
    "        for tr in latest_rows:\n",
    "            # read team_name_abbr text\n",
    "            tds = tr.xpath(\"./td[@data-stat='team_name_abbr']\")\n",
    "            team_txt = tds[0].xpath(\"normalize-space(string(.))\").upper() if tds else \"\"\n",
    "            if team_txt in aggregate_markers:\n",
    "                chosen = tr\n",
    "                break\n",
    "\n",
    "    need_manual_aggregate = (chosen is None and len(latest_rows) > 1)\n",
    "\n",
    "    # â”€â”€ output dict for current season â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # Name mapping rule inline\n",
    "    # (append '_per_minute_36' if not already present)\n",
    "    # We'll implement inline each time we set a key.\n",
    "\n",
    "    # â”€â”€ simple read path (have a chosen row or only one row) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if not need_manual_aggregate:\n",
    "        if chosen is None:\n",
    "            chosen = latest_rows[0]\n",
    "        # build map\n",
    "        tds = chosen.xpath(\"./td[@data-stat]\")\n",
    "        td_by_stat = {td.get(\"data-stat\"): td for td in tds}\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = td_by_stat.get(stat)\n",
    "            if td is None:\n",
    "                continue\n",
    "            val = td.xpath(\"normalize-space(string(.))\")\n",
    "            key = stat if \"per_minute_36\" in stat else f\"{stat}_per_minute_36\"\n",
    "            out[key] = val\n",
    "\n",
    "    # â”€â”€ manual aggregation path (minutes-weighted) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    else:\n",
    "        # helper inline parsing: convert a text to float safely\n",
    "        def _to_float_inline(s: str) -> Optional[float]:\n",
    "            s = (s or \"\").strip()\n",
    "            if not s:\n",
    "                return None\n",
    "            if s.startswith(\".\") and s[1:].replace(\".\", \"\", 1).isdigit():\n",
    "                s = \"0\" + s\n",
    "            try:\n",
    "                return float(s)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # filter out aggregate rows; keep per-team rows for the season\n",
    "        team_rows = []\n",
    "        for tr in latest_rows:\n",
    "            tds = tr.xpath(\"./td[@data-stat='team_name_abbr']\")\n",
    "            team_txt = tds[0].xpath(\"normalize-space(string(.))\").upper() if tds else \"\"\n",
    "            if team_txt not in aggregate_markers:\n",
    "                team_rows.append(tr)\n",
    "\n",
    "        # parse rows into numeric dicts + mp weights\n",
    "        parsed_rows: List[Dict[str, Optional[float]]] = []\n",
    "        for tr in team_rows:\n",
    "            row_dict: Dict[str, Optional[float]] = {}\n",
    "            # minutes weight from this table's MP\n",
    "            mp_td = tr.xpath(\"./td[@data-stat='mp']\")\n",
    "            mp_val = mp_td[0].xpath(\"normalize-space(string(.))\") if mp_td else \"\"\n",
    "            row_dict[\"mp\"] = _to_float_inline(mp_val) or 0.0\n",
    "            # parse candidates\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                td = tr.xpath(f\"./td[@data-stat='{stat}']\")\n",
    "                sval = td[0].xpath(\"normalize-space(string(.))\") if td else \"\"\n",
    "                row_dict[stat] = _to_float_inline(sval)\n",
    "            parsed_rows.append(row_dict)\n",
    "\n",
    "        total_mp = sum((r.get(\"mp\", 0.0) or 0.0) for r in parsed_rows)\n",
    "        if total_mp <= 0:\n",
    "            # fallback: use the first team row's strings\n",
    "            chosen = team_rows[0]\n",
    "            tds = chosen.xpath(\"./td[@data-stat]\")\n",
    "            td_by_stat = {td.get(\"data-stat\"): td for td in tds}\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                td = td_by_stat.get(stat)\n",
    "                if td is None:\n",
    "                    continue\n",
    "                val = td.xpath(\"normalize-space(string(.))\")\n",
    "                key = stat if \"per_minute_36\" in stat else f\"{stat}_per_minute_36\"\n",
    "                out[key] = val\n",
    "        else:\n",
    "            # weighted average by MP\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                num = 0.0\n",
    "                den = 0.0\n",
    "                for r in parsed_rows:\n",
    "                    v = r.get(stat)\n",
    "                    w = r.get(\"mp\", 0.0) or 0.0\n",
    "                    if v is None:\n",
    "                        continue\n",
    "                    num += v * w\n",
    "                    den += w\n",
    "                sval = \"\"\n",
    "                if den > 0:\n",
    "                    agg = num / den\n",
    "                    sval = f\"{agg:.3f}\".rstrip(\"0\").rstrip(\".\")\n",
    "                key = stat if \"per_minute_36\" in stat else f\"{stat}_per_minute_36\"\n",
    "                out[key] = sval\n",
    "\n",
    "    # â”€â”€ career row from <tfoot> (robust) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Strategy:\n",
    "    #   1) Try to find a <tr> whose TH (data-stat=\"year_id\") is exactly like \"^\\d+ Yrs$\"\n",
    "    #   2) Else, among all tfoot rows that *contain* \"<number> Yrs\", pick the largest number\n",
    "    tf_rows = table.xpath(\".//tfoot/tr\")\n",
    "    career_tr = None\n",
    "    if tf_rows:\n",
    "        # pass 1: exact match\n",
    "        for tr in tf_rows:\n",
    "            th_text = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "            if re.fullmatch(r\"\\d+\\s+Yr(?:s)?\", th_text or \"\"):\n",
    "                career_tr = tr\n",
    "                break\n",
    "        # pass 2: best (max) \"<n> Yrs\"\n",
    "        if career_tr is None:\n",
    "            best = None\n",
    "            best_years = -1\n",
    "            for tr in tf_rows:\n",
    "                th_text = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "                m = re.search(r\"(\\d+)\\s+Yr(?:s)?\", th_text or \"\")\n",
    "                if m:\n",
    "                    yrs = int(m.group(1))\n",
    "                    if yrs > best_years:\n",
    "                        best_years = yrs\n",
    "                        best = tr\n",
    "            career_tr = best\n",
    "\n",
    "    if career_tr is not None:\n",
    "        td_by_stat_c = {td.get(\"data-stat\"): td for td in career_tr.xpath(\"./td[@data-stat]\")}\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = td_by_stat_c.get(stat)\n",
    "            if td is None:\n",
    "                continue\n",
    "            val = td.xpath(\"normalize-space(string(.))\")\n",
    "            key = stat if \"per_minute_36\" in stat else f\"{stat}_per_minute_36\"\n",
    "            out[f\"career_{key}\"] = val\n",
    "\n",
    "    return out\n",
    "\n",
    "       \n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    #Parse the 'Per 100 Possessions' table (id='per_poss') and return the latest-season row.\n",
    "    #- Picks the current season (max csk). If multiple rows (traded), prefers the aggregate row\n",
    "    #  (TOT / 2TM / 3TM / 4TM) when prefer_tot=True.\n",
    "    #- EXCLUDES: season/year_id, age, team_id, lg_id, pos, g, gs, awards.\n",
    "    #- For any data-stat that doesn't already end with '_per_poss', append '_per_poss' to the key.\n",
    "    #- Preserves header leftâ†’right order.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def per_100_poss_stats(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    prefer_tot: bool = True,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    # â”€â”€ fetch & DOM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    # Primary id is 'per_poss'; keep 'per_poss_stats' as rare fallback\n",
    "    tables = dom.xpath(\"//table[@id='per_poss']\") or dom.xpath(\"//table[@id='per_poss_stats']\")\n",
    "    if not tables:\n",
    "        return None\n",
    "    table = tables[0]\n",
    "\n",
    "    # â”€â”€ header (leftâ†’right) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    header_stats: List[str] = []\n",
    "    ths = table.xpath(\".//thead//tr[1]/th\")\n",
    "    for th in ths:\n",
    "        ds = th.get(\"data-stat\")\n",
    "        if ds:\n",
    "            header_stats.append(ds)\n",
    "\n",
    "    # â”€â”€ body rows with a season cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    rows = table.xpath(\".//tbody/tr[th[@data-stat='year_id']]\")\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    # Build (csk_int, tr)\n",
    "    seasons: List[tuple[int, Any]] = []\n",
    "    for tr in rows:\n",
    "        th = tr.xpath(\"./th[@data-stat='year_id']\")\n",
    "        if not th:\n",
    "            continue\n",
    "        csk = th[0].get(\"csk\")\n",
    "        csk_int = 0\n",
    "        if csk is not None:\n",
    "            try:\n",
    "                csk_int = int(csk)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if csk_int == 0:\n",
    "            txt = th[0].xpath(\"normalize-space(string(.))\")\n",
    "            m = re.search(r\"(\\d{4})(?:-\\d{2})?$\", txt or \"\")\n",
    "            if m:\n",
    "                try:\n",
    "                    csk_int = int(m.group(1))\n",
    "                except ValueError:\n",
    "                    csk_int = 0\n",
    "        seasons.append((csk_int, tr))\n",
    "\n",
    "    if not seasons:\n",
    "        return None\n",
    "\n",
    "    max_csk = max(c for c, _ in seasons)\n",
    "    latest_rows = [tr for (c, tr) in seasons if c == max_csk]\n",
    "\n",
    "    # â”€â”€ config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    aggregate_markers = {\"TOT\", \"2TM\", \"3TM\", \"4TM\"}\n",
    "    exclude_stats = {\n",
    "        \"year_id\", \"age\", \"team_name_abbr\", \"comp_name_abbr\",\n",
    "        \"pos\", \"awards\"\n",
    "    }\n",
    "\n",
    "    # â”€â”€ prefer official aggregate row (TOT/2TM/3TM/4TM) if present â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    chosen = None\n",
    "    if prefer_tot and len(latest_rows) > 1:\n",
    "        for tr in latest_rows:\n",
    "            tds = tr.xpath(\"./td[@data-stat='team_name_abbr']\")\n",
    "            team_txt = tds[0].xpath(\"normalize-space(string(.))\").upper() if tds else \"\"\n",
    "            if team_txt in aggregate_markers:\n",
    "                chosen = tr\n",
    "                break\n",
    "\n",
    "    need_manual_aggregate = (chosen is None and len(latest_rows) > 1)\n",
    "\n",
    "    # â”€â”€ output dict for current season â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # â”€â”€ simple read path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if not need_manual_aggregate:\n",
    "        if chosen is None:\n",
    "            chosen = latest_rows[0]\n",
    "        td_by_stat = {td.get(\"data-stat\"): td for td in chosen.xpath(\"./td[@data-stat]\")}\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = td_by_stat.get(stat)\n",
    "            if td is None:\n",
    "                continue\n",
    "            val = td.xpath(\"normalize-space(string(.))\")\n",
    "            key = stat if stat.endswith(\"_per_poss\") else f\"{stat}_per_poss\"\n",
    "            out[key] = val\n",
    "    else:\n",
    "        # â”€â”€ manual aggregation (minutes-weighted) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        def _to_float_inline(s: str) -> Optional[float]:\n",
    "            s = (s or \"\").strip()\n",
    "            if not s:\n",
    "                return None\n",
    "            if s.startswith(\".\") and s[1:].replace(\".\", \"\", 1).isdigit():\n",
    "                s = \"0\" + s\n",
    "            try:\n",
    "                return float(s)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # keep only per-team rows (exclude TOT/2TM/3TM/4TM)\n",
    "        team_rows = []\n",
    "        for tr in latest_rows:\n",
    "            tds = tr.xpath(\"./td[@data-stat='team_name_abbr']\")\n",
    "            team_txt = tds[0].xpath(\"normalize-space(string(.))\").upper() if tds else \"\"\n",
    "            if team_txt not in aggregate_markers:\n",
    "                team_rows.append(tr)\n",
    "\n",
    "        parsed_rows: List[Dict[str, Optional[float]]] = []\n",
    "        for tr in team_rows:\n",
    "            row_dict: Dict[str, Optional[float]] = {}\n",
    "            # mp weight\n",
    "            mp_td = tr.xpath(\"./td[@data-stat='mp']\")\n",
    "            mp_val = mp_td[0].xpath(\"normalize-space(string(.))\") if mp_td else \"\"\n",
    "            row_dict[\"mp\"] = _to_float_inline(mp_val) or 0.0\n",
    "            # parse candidates\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                td = tr.xpath(f\"./td[@data-stat='{stat}']\")\n",
    "                sval = td[0].xpath(\"normalize-space(string(.))\") if td else \"\"\n",
    "                row_dict[stat] = _to_float_inline(sval)\n",
    "            parsed_rows.append(row_dict)\n",
    "\n",
    "        total_mp = sum((r.get(\"mp\", 0.0) or 0.0) for r in parsed_rows)\n",
    "        if total_mp <= 0:\n",
    "            # fallback: first team row (strings)\n",
    "            chosen = team_rows[0]\n",
    "            td_by_stat = {td.get(\"data-stat\"): td for td in chosen.xpath(\"./td[@data-stat]\")}\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                td = td_by_stat.get(stat)\n",
    "                if td is None:\n",
    "                    continue\n",
    "                val = td.xpath(\"normalize-space(string(.))\")\n",
    "                key = stat if stat.endswith(\"_per_poss\") else f\"{stat}_per_poss\"\n",
    "                out[key] = val\n",
    "        else:\n",
    "            # weighted averages\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                num = 0.0\n",
    "                den = 0.0\n",
    "                for r in parsed_rows:\n",
    "                    v = r.get(stat)\n",
    "                    w = r.get(\"mp\", 0.0) or 0.0\n",
    "                    if v is None:\n",
    "                        continue\n",
    "                    num += v * w\n",
    "                    den += w\n",
    "                sval = \"\"\n",
    "                if den > 0:\n",
    "                    agg = num / den\n",
    "                    sval = f\"{agg:.3f}\".rstrip(\"0\").rstrip(\".\")\n",
    "                key = stat if stat.endswith(\"_per_poss\") else f\"{stat}_per_poss\"\n",
    "                out[key] = sval\n",
    "\n",
    "    # â”€â”€ career row from <tfoot> (robust selection) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    tf_rows = table.xpath(\".//tfoot/tr\")\n",
    "    career_tr = None\n",
    "    if tf_rows:\n",
    "        # pass 1: exact \"\\d+ Yrs\"\n",
    "        for tr in tf_rows:\n",
    "            th_text = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "            if re.fullmatch(r\"\\d+\\s+Yr(?:s)?\", th_text or \"\"):\n",
    "                career_tr = tr\n",
    "                break\n",
    "        # pass 2: pick the tfoot with the largest \"<n> Yrs\"\n",
    "        if career_tr is None:\n",
    "            best = None\n",
    "            best_years = -1\n",
    "            for tr in tf_rows:\n",
    "                th_text = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "                m = re.search(r\"(\\d+)\\s+Yr(?:s)?\", th_text or \"\")\n",
    "                if m:\n",
    "                    yrs = int(m.group(1))\n",
    "                    if yrs > best_years:\n",
    "                        best_years = yrs\n",
    "                        best = tr\n",
    "            career_tr = best\n",
    "\n",
    "    if career_tr is not None:\n",
    "        td_by_stat_c = {td.get(\"data-stat\"): td for td in career_tr.xpath(\"./td[@data-stat]\")}\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = td_by_stat_c.get(stat)\n",
    "            if td is None:\n",
    "                continue\n",
    "            val = td.xpath(\"normalize-space(string(.))\")\n",
    "            key = stat if stat.endswith(\"_per_poss\") else f\"{stat}_per_poss\"\n",
    "            out[f\"career_{key}\"] = val\n",
    "\n",
    "    return out\n",
    "\n",
    "def advanced_stats(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    prefer_tot: bool = True,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse the 'Advanced' table (id='advanced'):\n",
    "      - Pick the latest season by max <th data-stat=\"year_id\" csk>.\n",
    "      - If multiple rows (player traded), prefer the aggregate row (TOT/2TM/3TM/4TM) when prefer_tot=True.\n",
    "      - If no aggregate row is available, manually aggregate per-team rows:\n",
    "          * minutes-weighted average for rate/percentage metrics\n",
    "          * simple sum for total-like metrics (ows, dws, ws, vorp)\n",
    "      - Exclude identity columns: year_id, age, team_name_abbr, comp_name_abbr, pos, awards\n",
    "      - Append `_adv` to ALL output metric keys.\n",
    "      - Also extract the 'career' row from <tfoot> (the one with 'Yr' or 'Yrs' in header) as `career_<stat>_adv`.\n",
    "      - If no career footer exists, synthesize `career_*_adv` from the latest-season values.\n",
    "    \"\"\"\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    tables = dom.xpath(\"//table[@id='advanced']\")\n",
    "    if not tables:\n",
    "        return None\n",
    "    table = tables[0]\n",
    "\n",
    "    # Header order (leftâ†’right)\n",
    "    header_stats: List[str] = []\n",
    "    for th in table.xpath(\".//thead//tr[1]/th\"):\n",
    "        ds = th.get(\"data-stat\")\n",
    "        if ds:\n",
    "            header_stats.append(ds)\n",
    "\n",
    "    # Body rows with a season cell\n",
    "    rows = table.xpath(\".//tbody/tr[th[@data-stat='year_id']]\")\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    # Latest season via csk\n",
    "    seasons: List[tuple[int, Any]] = []\n",
    "    for tr in rows:\n",
    "        th = tr.xpath(\"./th[@data-stat='year_id']\")[0]\n",
    "        csk = th.get(\"csk\")\n",
    "        try:\n",
    "            csk_int = int(csk)\n",
    "        except (TypeError, ValueError):\n",
    "            txt = th.xpath(\"normalize-space(string(.))\")\n",
    "            m = re.search(r\"(\\d{4})(?:-\\d{2})?$\", txt or \"\")\n",
    "            csk_int = int(m.group(1)) if m else 0\n",
    "        seasons.append((csk_int, tr))\n",
    "    max_csk = max(c for c, _ in seasons)\n",
    "    latest_rows = [tr for (c, tr) in seasons if c == max_csk]\n",
    "\n",
    "    # Helpers\n",
    "    def _cell_text(tr, stat: str) -> str:\n",
    "        if stat == \"year_id\":\n",
    "            th = tr.xpath(\"./th[@data-stat='year_id']\")\n",
    "            return th[0].xpath(\"normalize-space(string(.))\") if th else \"\"\n",
    "        td = tr.xpath(f\"./td[@data-stat='{stat}']\")\n",
    "        return td[0].xpath(\"normalize-space(string(.))\") if td else \"\"\n",
    "\n",
    "    def _to_float(s: str) -> Optional[float]:\n",
    "        s = (s or \"\").strip()\n",
    "        if not s:\n",
    "            return None\n",
    "        if s.startswith(\".\") and s[1:].replace(\".\", \"\", 1).isdigit():\n",
    "            s = \"0\" + s\n",
    "        try:\n",
    "            return float(s)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def _out_key(stat: str) -> str:\n",
    "        # every advanced metric gets _adv suffix\n",
    "        return f\"{stat}_adv\"\n",
    "\n",
    "    aggregate_markers = {\"TOT\", \"2TM\", \"3TM\", \"4TM\"}\n",
    "\n",
    "    # identity/metadata to exclude\n",
    "    exclude_stats = {\n",
    "        \"year_id\", \"age\", \"team_name_abbr\", \"comp_name_abbr\",\n",
    "        \"pos\", \"awards\"\n",
    "    }\n",
    "\n",
    "    # BBR season totals to SUM; everything else minutes-weighted\n",
    "    total_like = {\"ows\", \"dws\", \"ws\", \"vorp\"}\n",
    "\n",
    "    # Prefer official aggregate row\n",
    "    chosen = None\n",
    "    if prefer_tot and len(latest_rows) > 1:\n",
    "        for tr in latest_rows:\n",
    "            team_txt = (_cell_text(tr, \"team_name_abbr\") or \"\").upper()\n",
    "            if team_txt in aggregate_markers:\n",
    "                chosen = tr\n",
    "                break\n",
    "\n",
    "    need_manual_aggregate = chosen is None and len(latest_rows) > 1\n",
    "\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # Single/aggregate row path\n",
    "    if not need_manual_aggregate:\n",
    "        if chosen is None:\n",
    "            chosen = latest_rows[0]\n",
    "\n",
    "        td_by_stat = {td.get(\"data-stat\"): td for td in chosen.xpath(\"./td[@data-stat]\")}\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = td_by_stat.get(stat)\n",
    "            if td is None:\n",
    "                continue\n",
    "            out[_out_key(stat)] = td.xpath(\"normalize-space(string(.))\")\n",
    "\n",
    "    else:\n",
    "        # Manual aggregation (minutes-weighted for rates; sum for totals)\n",
    "        team_rows = []\n",
    "        for tr in latest_rows:\n",
    "            team_txt = (_cell_text(tr, \"team_name_abbr\") or \"\").upper()\n",
    "            if team_txt not in aggregate_markers:\n",
    "                team_rows.append(tr)\n",
    "        if not team_rows:\n",
    "            team_rows = [latest_rows[0]]\n",
    "\n",
    "        parsed_rows: List[Dict[str, Optional[float]]] = []\n",
    "        for tr in team_rows:\n",
    "            row: Dict[str, Optional[float]] = {}\n",
    "            row[\"mp\"] = _to_float(_cell_text(tr, \"mp\")) or 0.0\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                row[stat] = _to_float(_cell_text(tr, stat))\n",
    "            parsed_rows.append(row)\n",
    "\n",
    "        total_mp = sum((r.get(\"mp\") or 0.0) for r in parsed_rows)\n",
    "\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "\n",
    "            if stat in total_like:\n",
    "                s = 0.0\n",
    "                have_any = False\n",
    "                for r in parsed_rows:\n",
    "                    v = r.get(stat)\n",
    "                    if v is not None:\n",
    "                        s += v\n",
    "                        have_any = True\n",
    "                out[_out_key(stat)] = (f\"{s:.3f}\".rstrip(\"0\").rstrip(\".\") if have_any else \"\")\n",
    "            else:\n",
    "                if total_mp <= 0:\n",
    "                    out[_out_key(stat)] = _cell_text(team_rows[0], stat)\n",
    "                else:\n",
    "                    num = 0.0\n",
    "                    for r in parsed_rows:\n",
    "                        v = r.get(stat)\n",
    "                        w = r.get(\"mp\") or 0.0\n",
    "                        if v is None:\n",
    "                            continue\n",
    "                        num += v * w\n",
    "                    agg = num / total_mp\n",
    "                    out[_out_key(stat)] = f\"{agg:.3f}\".rstrip(\"0\").rstrip(\".\")\n",
    "\n",
    "    # â”€â”€ Career from <tfoot> (accept \"1 Yr\" or \"N Yrs\"); else synthesize â”€â”€\n",
    "    career_row = None\n",
    "    tf_rows = table.xpath(\".//tfoot/tr\")\n",
    "    if tf_rows:\n",
    "        # pass 1: exact \"N Yr\" or \"N Yrs\"\n",
    "        for tr in tf_rows:\n",
    "            label = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "            if re.fullmatch(r\"\\d+\\s+Yr(?:s)?\", label or \"\"):\n",
    "                career_row = tr\n",
    "                break\n",
    "        # pass 2: pick the tfoot with the largest \"<n> Yr(s)\"\n",
    "        if career_row is None:\n",
    "            best = None\n",
    "            best_years = -1\n",
    "            for tr in tf_rows:\n",
    "                label = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "                m = re.search(r\"(\\d+)\\s+Yr(?:s)?\", label or \"\")\n",
    "                if m:\n",
    "                    yrs = int(m.group(1))\n",
    "                    if yrs > best_years:\n",
    "                        best_years = yrs\n",
    "                        best = tr\n",
    "            career_row = best\n",
    "\n",
    "    if career_row is not None:\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = career_row.xpath(f\"./td[@data-stat='{stat}']\")\n",
    "            if not td:\n",
    "                continue\n",
    "            val = td[0].xpath(\"normalize-space(string(.))\")\n",
    "            out[f\"career_{stat}_adv\"] = val\n",
    "    else:\n",
    "        # No footer: synthesize career_* from latest-season values we already computed\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            key = _out_key(stat)  # e.g., 'ts_pct_adv'\n",
    "            if key in out:\n",
    "                out[f\"career_{stat}_adv\"] = out[key]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def player_salary(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse the Salaries table (id='all_salaries') and return only the current season salary.\n",
    "    Example output:\n",
    "        { \"salary\": \"$17,260,000\" }\n",
    "    \"\"\"\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    table_nodes = dom.xpath(\"//table[@id='all_salaries']\")\n",
    "    if not table_nodes:\n",
    "        return None\n",
    "    table = table_nodes[0]\n",
    "\n",
    "    # Helper: convert \"2024-25\" â†’ 2025\n",
    "    def _season_end_year(season_text: str) -> int:\n",
    "        s = (season_text or \"\").strip()\n",
    "        m = re.match(r\"^(\\d{4})(?:-(\\d{2}))?$\", s)\n",
    "        if not m:\n",
    "            return 0\n",
    "        start_year = int(m.group(1))\n",
    "        return start_year + 1 if m.group(2) else start_year\n",
    "\n",
    "    rows = table.xpath(\".//tbody/tr[th[@data-stat='season']]\")\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    # Pick the latest season row\n",
    "    best = None\n",
    "    best_end_year = -1\n",
    "    for tr in rows:\n",
    "        th = tr.xpath(\"./th[@data-stat='season']\")\n",
    "        season_text = th[0].xpath(\"normalize-space(string(.))\") if th else \"\"\n",
    "        end_year = _season_end_year(season_text)\n",
    "        if end_year > best_end_year:\n",
    "            best_end_year = end_year\n",
    "            best = tr\n",
    "\n",
    "    if best is None:\n",
    "        return None\n",
    "\n",
    "    # Get salary text from that row\n",
    "    sal_td = best.xpath(\"./td[@data-stat='salary']\")\n",
    "    salary_text = sal_td[0].xpath(\"normalize-space(string(.))\") if sal_td else \"\"\n",
    "    return {\"salary\": salary_text}\n",
    "\n",
    "\n",
    "\n",
    "def player_current_contract(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Current contract salary from a contracts_* table:\n",
    "      - Find first contracts_* table (e.g., contracts_lac, contracts_pho)\n",
    "      - Take the FIRST row\n",
    "      - Skip the Team cell, read the FIRST salary cell\n",
    "      - Return {'current_contract': '$X,XXX,XXX'} or blank if unavailable\n",
    "    \"\"\"\n",
    "    # Ensure we have a DOM that already unhides commented tables\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    # 1) DOM path: any table with id beginning with \"contracts_\"\n",
    "    tables = dom.xpath(\"//table[starts-with(@id,'contracts_')]\")\n",
    "    table = tables[0] if tables else None\n",
    "\n",
    "    # 2) Fallback: regex scan raw HTML (first contracts_* table)\n",
    "    if table is None and html_text:\n",
    "        m = re.search(r\"(<table[^>]+id=[\\\"']?contracts_[^>]*>.*?</table>)\",\n",
    "                      html_text, re.DOTALL | re.IGNORECASE)\n",
    "        if m:\n",
    "            try:\n",
    "                table = html.fromstring(m.group(1))\n",
    "            except Exception:\n",
    "                table = None\n",
    "\n",
    "    # Nothing found â†’ blank\n",
    "    if table is None:\n",
    "        return {\"current_contract\": \"\"}\n",
    "\n",
    "    # Find the first data row\n",
    "    rows = table.xpath(\".//tbody/tr\") or table.xpath(\".//tr[td]\")\n",
    "    if not rows:\n",
    "        return {\"current_contract\": \"\"}\n",
    "\n",
    "    first_row = rows[0]\n",
    "\n",
    "    # Cells: [0]=Team, [1]=first season salary (the one we want)\n",
    "    tds = first_row.xpath(\"./td\")\n",
    "    if len(tds) < 2:\n",
    "        return {\"current_contract\": \"\"}\n",
    "\n",
    "    # Read the first salary cell (skip the Team cell)\n",
    "    salary_text = tds[1].xpath(\"normalize-space(string(.))\") or \"\"\n",
    "    return {\"current_contract\": salary_text}\n",
    "\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# URL reader (absolute path per your setup) + CSV writer (preserve order)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _read_urls_from_file(\n",
    "    txt_path: str = \"data/players.txt\"\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads player URLs from file and normalizes them.\n",
    "    Accepts:\n",
    "      - full URLs (http/https, with/without www, with ?/# â†’ stripped)\n",
    "      - paths like /players/t/tatumja01[.html]\n",
    "      - bare ids like tatumja01  (infer folder)\n",
    "    De-duplicates while preserving order.\n",
    "    Prints diagnostics + shows duplicates & invalid entries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸ players file not found: {txt_path}\")\n",
    "        return []\n",
    "\n",
    "    def _normalize(token: str) -> Optional[str]:\n",
    "        token = token.strip()\n",
    "        if not token:\n",
    "            return None\n",
    "        token = token.split(\"#\", 1)[0].strip()  # remove inline comments\n",
    "        if not token:\n",
    "            return None\n",
    "\n",
    "        # bare id like 'tatumja01'\n",
    "        if re.fullmatch(r\"[A-Za-z][A-Za-z0-9]{8,}\", token):\n",
    "            pid = token.lower()\n",
    "            return f\"https://www.basketball-reference.com/players/{pid[0]}/{pid}.html\"\n",
    "\n",
    "        # path like /players/t/tatumja01\n",
    "        if re.fullmatch(r\"/?players/[A-Za-z]/[A-Za-z0-9]+(?:\\.html)?/?\", token):\n",
    "            path = token if token.startswith(\"/\") else \"/\" + token\n",
    "            if not path.endswith(\".html\"):\n",
    "                path = path.rstrip(\"/\") + \".html\"\n",
    "            parts = path.split(\"/\")\n",
    "            parts[-1] = parts[-1].lower()\n",
    "            parts[-2] = parts[-2].lower()\n",
    "            return \"https://www.basketball-reference.com\" + \"/\".join(parts)\n",
    "\n",
    "        # URL without scheme\n",
    "        if token.startswith(\"www.basketball-reference.com/\"):\n",
    "            token = \"https://\" + token\n",
    "\n",
    "        # full URL\n",
    "        if token.startswith((\"http://\", \"https://\")):\n",
    "            m = re.match(r\"^(https?://)(?:www\\.)?basketball-reference\\.com([^?#]*)\", token, re.I)\n",
    "            if not m:\n",
    "                return None\n",
    "            path = m.group(2)\n",
    "            if not path.startswith(\"/\"):\n",
    "                path = \"/\" + path\n",
    "            m2 = re.match(r\"^/players/([A-Za-z])/([A-Za-z0-9]+)(?:\\.html)?/?$\", path)\n",
    "            if not m2:\n",
    "                return None\n",
    "            return f\"https://www.basketball-reference.com/players/{m2.group(1).lower()}/{m2.group(2).lower()}.html\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    tokens = re.split(r\"[\\s,]+\", raw.strip())\n",
    "    normalized, invalid_samples = [], []\n",
    "    total_tokens = 0\n",
    "    for t in tokens:\n",
    "        if not t:\n",
    "            continue\n",
    "        total_tokens += 1\n",
    "        u = _normalize(t)\n",
    "        if u:\n",
    "            normalized.append(u)\n",
    "        else:\n",
    "            if len(invalid_samples) < 10:\n",
    "                invalid_samples.append(t)\n",
    "\n",
    "    # de-dup but capture duplicates\n",
    "    seen, unique_urls, duplicates = set(), [], []\n",
    "    for u in normalized:\n",
    "        if u in seen:\n",
    "            duplicates.append(u)\n",
    "        else:\n",
    "            seen.add(u)\n",
    "            unique_urls.append(u)\n",
    "\n",
    "    # diagnostics\n",
    "    num_valid = len(normalized)\n",
    "    num_unique = len(unique_urls)\n",
    "    num_dupes = len(duplicates)\n",
    "    num_invalid = total_tokens - num_valid\n",
    "\n",
    "    print(\n",
    "        f\"ðŸ”Ž Parsed {total_tokens} entries â†’ valid {num_valid}, \"\n",
    "        f\"duplicates {num_dupes}, invalid {num_invalid}.\"\n",
    "    )\n",
    "    print(f\"ðŸ“„ Using {num_unique} unique URL(s) from {txt_path}\")\n",
    "\n",
    "    if duplicates:\n",
    "        print(\"âš ï¸ Duplicate entries (player appeared multiple times):\")\n",
    "        for d in duplicates[:20]:  # limit to first 20\n",
    "            print(f\"   - {d}\")\n",
    "        if len(duplicates) > 20:\n",
    "            print(f\"   ... and {len(duplicates)-20} more\")\n",
    "\n",
    "    if invalid_samples:\n",
    "        print(\"âš ï¸ Sample invalid entries (first 10):\")\n",
    "        for s in invalid_samples:\n",
    "            print(f\"   - {s}\")\n",
    "\n",
    "    return unique_urls\n",
    "\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Runner (logging + CSV)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def basketball_ref_all_players_stats(\n",
    "    urls: Optional[List[str]] = None,\n",
    "    api_key: Optional[str] = None,\n",
    "    csv_path: str = \"output_data/all_nba_players_stats.csv\",\n",
    "    prefer_tot: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Scrape players and stream results to CSV row-by-row, combining:\n",
    "      - players_personal_info()\n",
    "      - player_salary()              -> 'salary' (current-season *paid* salary table)\n",
    "      - current_contract_salary()    -> 'current_contract_salary' (contracts table for detected season)\n",
    "      - per_game_stats()\n",
    "      - per_36_min_stats()\n",
    "      - per_100_poss_stats()\n",
    "      - advanced_stats()\n",
    "    \"\"\"\n",
    "    if urls is None:\n",
    "        urls = _read_urls_from_file()\n",
    "\n",
    "    if not urls:\n",
    "        print(\"ðŸš« No player URLs to process. Provide `urls=[...]` or put them in data/players.txt\")\n",
    "        return\n",
    "\n",
    "    total = len(urls)\n",
    "\n",
    "    _ensure_dir(os.path.dirname(csv_path) or \".\")\n",
    "    fieldnames = None\n",
    "    file = open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\")\n",
    "    writer = None\n",
    "\n",
    "    try:\n",
    "        for i, url in enumerate(urls, 1):\n",
    "            try:\n",
    "                html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "                dom = _build_dom(html_text)\n",
    "\n",
    "                personal  = players_personal_info(url, api_key=api_key, dom=dom, html_text=html_text)\n",
    "                salary    = player_salary(url, api_key=api_key, dom=dom, html_text=html_text)                  # {'salary': '...'}\n",
    "                contract = player_current_contract(url, api_key=api_key, dom=dom, html_text=html_text)   # {'current_contract_salary': '...'}\n",
    "                pergame   = per_game_stats(url, api_key=api_key, dom=dom, html_text=html_text)\n",
    "                per36     = per_36_min_stats(url, api_key=api_key, prefer_tot=prefer_tot, dom=dom, html_text=html_text)\n",
    "                per100    = per_100_poss_stats(url, api_key=api_key, prefer_tot=prefer_tot, dom=dom, html_text=html_text)\n",
    "                advanced  = advanced_stats(url, api_key=api_key, prefer_tot=prefer_tot, dom=dom, html_text=html_text)\n",
    "\n",
    "                # Merge row\n",
    "                combined = dict(personal)\n",
    "                if salary:\n",
    "                    combined.update(salary)              # adds 'salary'\n",
    "                if contract:\n",
    "                    combined.update(contract)            # adds 'current_contract'\n",
    "                if pergame:\n",
    "                    combined.update(pergame)\n",
    "                if per36:\n",
    "                    combined.update(per36)\n",
    "                if per100:\n",
    "                    combined.update(per100)\n",
    "                if advanced:\n",
    "                    combined.update(advanced)\n",
    "\n",
    "                combined.setdefault(\"url\", url)\n",
    "\n",
    "                # Initialize header order once (keep your established order)\n",
    "                if fieldnames is None:\n",
    "                    personal_first = [\n",
    "                        \"player_id\",\"player_name\",\"team\",\"birth_day\",\"years_experience\",\n",
    "                        \"salary\",                     # from player_salary()\n",
    "                        \"current_contract\",    # from current_contract()\n",
    "                        \"season\",\"age\",\"team_id\",\"lg_id\",\"pos\",\"g\",\"gs\",\n",
    "                        \"was_traded\",\"teams_count\",\"teams_played\"\n",
    "                    ]\n",
    "\n",
    "                    pergame_cols = []\n",
    "                    if pergame:\n",
    "                        for k in pergame.keys():\n",
    "                            if k not in personal_first:\n",
    "                                pergame_cols.append(k)\n",
    "\n",
    "                    per36_cols     = list(per36.keys()) if per36 else []\n",
    "                    per100_cols    = list(per100.keys()) if per100 else []\n",
    "                    advanced_cols  = list(advanced.keys()) if advanced else []\n",
    "\n",
    "                    fieldnames = personal_first + pergame_cols + per36_cols + per100_cols + advanced_cols\n",
    "\n",
    "                    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "\n",
    "                # Write row\n",
    "                writer.writerow({k: combined.get(k, \"\") for k in fieldnames})\n",
    "                file.flush()\n",
    "\n",
    "                print(f\"[{i}/{total}] âœ… {combined.get('player_id')} â€” written\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{i}/{total}] ðŸ’¥ Error for {url}: {e}\")\n",
    "\n",
    "            # time.sleep(random.uniform(0.2, 0.6))\n",
    "    finally:\n",
    "        file.close()\n",
    "\n",
    "    print(f\"ðŸ“ Finished. CSV written to {csv_path}\")\n",
    "\n",
    "               \n",
    "if __name__ == \"__main__\":\n",
    "    basketball_ref_all_players_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b3788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
