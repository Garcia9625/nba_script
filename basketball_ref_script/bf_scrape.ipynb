{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7a2396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Parsed 655 entries → valid 655, duplicates 86, invalid 0.\n",
      "📄 Using 569 unique URL(s) from data/players.txt\n",
      "⚠️ Duplicate entries (player appeared multiple times):\n",
      "   - https://www.basketball-reference.com/players/c/craigto01.html\n",
      "   - https://www.basketball-reference.com/players/h/huntede01.html\n",
      "   - https://www.basketball-reference.com/players/l/leverca01.html\n",
      "   - https://www.basketball-reference.com/players/n/niangge01.html\n",
      "   - https://www.basketball-reference.com/players/m/moorewe01.html\n",
      "   - https://www.basketball-reference.com/players/s/schrode01.html\n",
      "   - https://www.basketball-reference.com/players/b/beekmre01.html\n",
      "   - https://www.basketball-reference.com/players/s/schrode01.html\n",
      "   - https://www.basketball-reference.com/players/w/waterli01.html\n",
      "   - https://www.basketball-reference.com/players/r/roddyda01.html\n",
      "   - https://www.basketball-reference.com/players/b/brownmo01.html\n",
      "   - https://www.basketball-reference.com/players/b/bogdabo01.html\n",
      "   - https://www.basketball-reference.com/players/j/joneska01.html\n",
      "   - https://www.basketball-reference.com/players/m/mannte01.html\n",
      "   - https://www.basketball-reference.com/players/s/simmobe01.html\n",
      "   - https://www.basketball-reference.com/players/c/chrisma02.html\n",
      "   - https://www.basketball-reference.com/players/d/davisan02.html\n",
      "   - https://www.basketball-reference.com/players/d/doncilu01.html\n",
      "   - https://www.basketball-reference.com/players/f/finnedo01.html\n",
      "   - https://www.basketball-reference.com/players/k/klebima01.html\n",
      "   ... and 66 more\n",
      "[1/569] ✅ barlodo01 — written\n",
      "[2/569] ✅ bogdabo01 — written\n",
      "[3/569] ✅ bufkiko01 — written\n",
      "[4/569] ✅ capelca01 — written\n",
      "[5/569] ✅ daniedy01 — written\n",
      "[6/569] ✅ gueyemo02 — written\n",
      "[7/569] ✅ huntede01 — written\n",
      "[8/569] ✅ johnsja05 — written\n",
      "[9/569] ✅ krejcvi01 — written\n",
      "[10/569] ✅ leverca01 — written\n",
      "[11/569] ✅ mannte01 — written\n",
      "[12/569] ✅ mathega01 — written\n",
      "[13/569] ✅ nancela02 — written\n",
      "[14/569] ✅ niangge01 — written\n",
      "[15/569] ✅ okongon01 — written\n",
      "[16/569] ✅ plowdda01 — written\n",
      "[17/569] ✅ risacza01 — written\n",
      "[18/569] ✅ roddyda01 — written\n",
      "[19/569] ✅ toppija01 — written\n",
      "[20/569] ✅ wallake01 — written\n",
      "[21/569] ✅ youngtr01 — written\n",
      "[22/569] ✅ brownja02 — written\n",
      "[23/569] ✅ craigto01 — written\n",
      "[24/569] ✅ davisjd01 — written\n",
      "[25/569] ✅ hausesa01 — written\n",
      "[26/569] ✅ holidjr01 — written\n",
      "[27/569] ✅ horfoal01 — written\n",
      "[28/569] ✅ kornelu01 — written\n",
      "[29/569] ✅ norrimi01 — written\n",
      "[30/569] ✅ peterdr01 — written\n",
      "[31/569] ✅ porzikr01 — written\n",
      "[32/569] ✅ pritcpa01 — written\n",
      "[33/569] ✅ quetane01 — written\n",
      "[34/569] ✅ scheiba01 — written\n",
      "[35/569] ✅ sprinja01 — written\n",
      "[36/569] ✅ tatumja01 — written\n",
      "[37/569] ✅ tillmxa01 — written\n",
      "[38/569] ✅ walshjo01 — written\n",
      "[39/569] ✅ whitede01 — written\n",
      "[40/569] ✅ beekmre01 — written\n",
      "[41/569] ✅ claxtni01 — written\n",
      "[42/569] ✅ clownno01 — written\n",
      "[43/569] ✅ cuiyo01 — written\n",
      "[44/569] ✅ etienty01 — written\n",
      "[45/569] ✅ evbuoto01 — written\n",
      "[46/569] ✅ finnedo01 — written\n",
      "[47/569] ✅ hayeski01 — written\n",
      "[48/569] ✅ johnsca02 — written\n",
      "[49/569] ✅ johnske07 — written\n",
      "[50/569] ✅ lewisma05 — written\n",
      "[51/569] ✅ martija02 — written\n",
      "[52/569] ✅ martity01 — written\n",
      "[53/569] ✅ whiteda01 — written\n",
      "[54/569] ✅ miltosh01 — written\n",
      "[55/569] ✅ russeda01 — written\n",
      "[56/569] ✅ schrode01 — written\n",
      "[57/569] ✅ sharpda01 — written\n",
      "[58/569] ✅ simmobe01 — written\n",
      "[59/569] ✅ thomaca02 — written\n",
      "[60/569] ✅ timmedr01 — written\n",
      "[61/569] ✅ watfotr01 — written\n",
      "[62/569] ✅ willizi02 — written\n",
      "[63/569] ✅ wilsoja03 — written\n",
      "[64/569] ✅ balllo01 — written\n",
      "[65/569] ✅ buzelma01 — written\n",
      "[66/569] ✅ carteje01 — written\n",
      "[67/569] ✅ colliza01 — written\n",
      "[68/569] ✅ dosunay01 — written\n",
      "[69/569] ✅ duartch01 — written\n",
      "[70/569] ✅ giddejo01 — written\n",
      "[71/569] ✅ hortota01 — written\n",
      "[72/569] ✅ huertke01 — written\n",
      "[73/569] ✅ jonestr01 — written\n",
      "[74/569] ✅ lavinza01 — written\n",
      "[75/569] ✅ liddeej01 — written\n",
      "[76/569] ✅ milleem01 — written\n",
      "[77/569] ✅ phillju01 — written\n",
      "[78/569] ✅ sanogad01 — written\n",
      "[79/569] ✅ smithja04 — written\n",
      "[80/569] ✅ terryda01 — written\n",
      "[81/569] ✅ vucevni01 — written\n",
      "[82/569] ✅ whiteco01 — written\n",
      "[83/569] ✅ willipa01 — written\n",
      "[84/569] ✅ youngja05 — written\n",
      "[85/569] ✅ ballla01 — written\n",
      "[86/569] ✅ baughda01 — written\n",
      "[87/569] ✅ bridgmi02 — written\n",
      "[88/569] ✅ curryse01 — written\n",
      "[89/569] ✅ diabamo01 — written\n",
      "[90/569] ✅ flynnma01 — written\n",
      "[91/569] ✅ garrema01 — written\n",
      "[92/569] ✅ gibsota01 — written\n",
      "[93/569] ✅ greenjo02 — written\n",
      "[94/569] ✅ jeffrda01 — written\n",
      "[95/569] ✅ manntr01 — written\n",
      "[96/569] ✅ martico01 — written\n",
      "[97/569] ✅ micicva01 — written\n",
      "[98/569] ✅ millebr02 — written\n",
      "[99/569] ✅ moorewe01 — written\n",
      "[100/569] ✅ nurkiju01 — written\n",
      "[101/569] ✅ okogijo01 — written\n",
      "[102/569] ✅ paytoel01 — written\n",
      "[103/569] ✅ rhodeja01 — written\n",
      "[104/569] ✅ richani01 — written\n",
      "[105/569] ✅ salauti01 — written\n",
      "[106/569] ✅ simpskj01 — written\n",
      "[107/569] ✅ simsja01 — written\n",
      "[108/569] ✅ smithni01 — written\n",
      "[109/569] ✅ willigr01 — written\n",
      "[110/569] ✅ willima07 — written\n",
      "[111/569] ✅ wongis01 — written\n",
      "[112/569] ✅ allenja01 — written\n",
      "[113/569] ✅ batesem01 — written\n",
      "[114/569] ✅ garlada01 — written\n",
      "[115/569] ✅ greenja02 — written\n",
      "[116/569] ✅ jeromty01 — written\n",
      "[117/569] ✅ merrisa01 — written\n",
      "[118/569] ✅ mitchdo01 — written\n",
      "[119/569] ✅ mobleev01 — written\n",
      "[120/569] ✅ okekech01 — written\n",
      "[121/569] ✅ okorois01 — written\n",
      "[122/569] ✅ portecr01 — written\n",
      "[123/569] ✅ strusma01 — written\n",
      "[124/569] ✅ thomptr01 — written\n",
      "[125/569] ✅ thorjt01 — written\n",
      "[126/569] ✅ tomlina01 — written\n",
      "[127/569] ✅ travelu01 — written\n",
      "[128/569] ✅ tysonja01 — written\n",
      "[129/569] ✅ wadede01 — written\n",
      "[130/569] ✅ brownmo01 — written\n",
      "[131/569] ✅ chrisma02 — written\n",
      "[132/569] ✅ davisan02 — written\n",
      "[133/569] ✅ dinwisp01 — written\n",
      "[134/569] ✅ doncilu01 — written\n",
      "[135/569] ✅ edwarke02 — written\n",
      "[136/569] ✅ exumda01 — written\n",
      "[137/569] ✅ gaffoda01 — written\n",
      "[138/569] ✅ gortmja01 — written\n",
      "[139/569] ✅ grimequ01 — written\n",
      "[140/569] ✅ hardyja02 — written\n",
      "[141/569] ✅ irvinky01 — written\n",
      "[142/569] ✅ joneska01 — written\n",
      "[143/569] ✅ kelleky01 — written\n",
      "[144/569] ✅ klebima01 — written\n",
      "[145/569] ✅ livelde01 — written\n",
      "[146/569] ✅ marshna01 — written\n",
      "[147/569] ✅ martica02 — written\n",
      "[148/569] ✅ morrima02 — written\n",
      "[149/569] ✅ poweldw01 — written\n",
      "[150/569] ✅ prospol01 — written\n",
      "[151/569] ✅ thompkl01 — written\n",
      "[152/569] ✅ washipj01 — written\n",
      "[153/569] ✅ willibr03 — written\n",
      "[154/569] ✅ alexatr01 — written\n",
      "[155/569] ✅ braunch01 — written\n",
      "[156/569] ✅ cancavl01 — written\n",
      "[157/569] ✅ gordoaa01 — written\n",
      "[158/569] ✅ hallpj01 — written\n",
      "[159/569] ✅ jokicni01 — written\n",
      "[160/569] ✅ jonessp01 — written\n",
      "[161/569] ✅ jordade01 — written\n",
      "[162/569] ✅ murraja01 — written\n",
      "[163/569] ✅ nnajize01 — written\n",
      "[164/569] ✅ pickeja02 — written\n",
      "[165/569] ✅ portemi01 — written\n",
      "[166/569] ✅ saricda01 — written\n",
      "[167/569] ✅ strawju01 — written\n",
      "[168/569] ✅ tysonhu01 — written\n",
      "[169/569] ✅ watsope01 — written\n",
      "[170/569] ✅ westbru01 — written\n",
      "[171/569] ✅ beaslma01 — written\n",
      "[172/569] ✅ cunnica01 — written\n",
      "[173/569] ✅ durenja01 — written\n",
      "[174/569] ✅ fontesi01 — written\n",
      "[175/569] ✅ hardati02 — written\n",
      "[176/569] ✅ harpero02 — written\n",
      "[177/569] ✅ harrito02 — written\n",
      "[178/569] ✅ hollaro01 — written\n",
      "[179/569] ✅ iveyja01 — written\n",
      "[180/569] ✅ jenkida01 — written\n",
      "[181/569] ✅ klintbo01 — written\n",
      "[182/569] ✅ reedpa01 — written\n",
      "[183/569] ✅ sassema01 — written\n",
      "[184/569] ✅ smithto05 — written\n",
      "[185/569] ✅ stewais01 — written\n",
      "[186/569] ✅ swideco01 — written\n",
      "[187/569] ✅ thompau01 — written\n",
      "[188/569] ✅ waterli01 — written\n",
      "[189/569] ✅ willial06 — written\n",
      "[190/569] ✅ anderky01 — written\n",
      "[191/569] ✅ butleji01 — written\n",
      "[192/569] ✅ colliyu01 — written\n",
      "[193/569] ✅ curryst01 — written\n",
      "[194/569] ✅ greendr01 — written\n",
      "[195/569] ✅ hieldbu01 — written\n",
      "[196/569] ✅ jackstr02 — written\n",
      "[197/569] ✅ keybr01 — written\n",
      "[198/569] ✅ knoxke01 — written\n",
      "[199/569] ✅ kuminjo01 — written\n",
      "[200/569] ✅ looneke01 — written\n",
      "[201/569] ✅ meltode01 — written\n",
      "[202/569] ✅ moodymo01 — written\n",
      "[203/569] ✅ paytoga02 — written\n",
      "[204/569] ✅ podzibr01 — written\n",
      "[205/569] ✅ postqu01 — written\n",
      "[206/569] ✅ roweja01 — written\n",
      "[207/569] ✅ santogu01 — written\n",
      "[208/569] ✅ spencpa01 — written\n",
      "[209/569] ✅ wiggian01 — written\n",
      "[210/569] ✅ adamsst01 — written\n",
      "[211/569] ✅ brookdi01 — written\n",
      "[212/569] ✅ nfalyda01 — written\n",
      "[213/569] ✅ easonta01 — written\n",
      "[214/569] ✅ greenja05 — written\n",
      "[215/569] ✅ greenje02 — written\n",
      "[216/569] ✅ holidaa01 — written\n",
      "[217/569] ✅ landajo01 — written\n",
      "[218/569] ✅ mcveija01 — written\n",
      "[219/569] ✅ sengual01 — written\n",
      "[220/569] ✅ sheppre01 — written\n",
      "[221/569] ✅ smithja05 — written\n",
      "[222/569] ✅ tateja01 — written\n",
      "[223/569] ✅ thompam01 — written\n",
      "[224/569] ✅ vanvlfr01 — written\n",
      "[225/569] ✅ whitmca01 — written\n",
      "[226/569] ✅ willije02 — written\n",
      "[227/569] ✅ bradlto01 — written\n",
      "[228/569] ✅ bryanth01 — written\n",
      "[229/569] ✅ dennira01 — written\n",
      "[230/569] ✅ freemen01 — written\n",
      "[231/569] ✅ furphjo01 — written\n",
      "[232/569] ✅ halibty01 — written\n",
      "[233/569] ✅ jacksis01 — written\n",
      "[234/569] ✅ jacksqu01 — written\n",
      "[235/569] ✅ johnsja01 — written\n",
      "[236/569] ✅ mathube01 — written\n",
      "[237/569] ✅ mccontj01 — written\n",
      "[238/569] ✅ nembhan01 — written\n",
      "[239/569] ✅ nesmiaa01 — written\n",
      "[240/569] ✅ newtotr01 — written\n",
      "[241/569] ✅ okafoja01 — written\n",
      "[242/569] ✅ sheppbe01 — written\n",
      "[243/569] ✅ siakapa01 — written\n",
      "[244/569] ✅ toppiob01 — written\n",
      "[245/569] ✅ turnemy01 — written\n",
      "[246/569] ✅ walkeja02 — written\n",
      "[247/569] ✅ wisemja01 — written\n",
      "[248/569] ✅ baldwpa01 — written\n",
      "[249/569] ✅ bambamo01 — written\n",
      "[250/569] ✅ batumni01 — written\n",
      "[251/569] ✅ beaucma01 — written\n",
      "[252/569] ✅ brownko01 — written\n",
      "[253/569] ✅ chrisca02 — written\n",
      "[254/569] ✅ coffeam01 — written\n",
      "[255/569] ✅ dunnkr01 — written\n",
      "[256/569] ✅ eubandr01 — written\n",
      "[257/569] ✅ flowetr01 — written\n",
      "[258/569] ✅ hardeja01 — written\n",
      "[259/569] ✅ hylanbo01 — written\n",
      "[260/569] ✅ jonesde02 — written\n",
      "[261/569] ✅ leonaka01 — written\n",
      "[262/569] ✅ millejo02 — written\n",
      "[263/569] ✅ millspa02 — written\n",
      "[264/569] ✅ porteke02 — written\n",
      "[265/569] ✅ powelno01 — written\n",
      "[266/569] ✅ zubaciv01 — written\n",
      "[267/569] ✅ goodwjo01 — written\n",
      "[268/569] ✅ hachiru01 — written\n",
      "[269/569] ✅ hayesja02 — written\n",
      "[270/569] ✅ hoodsja01 — written\n",
      "[271/569] ✅ jamesbr02 — written\n",
      "[272/569] ✅ jamesle01 — written\n",
      "[273/569] ✅ jemistr01 — written\n",
      "[274/569] ✅ knechda01 — written\n",
      "[275/569] ✅ kolokch01 — written\n",
      "[276/569] ✅ lenal01 — written\n",
      "[277/569] ✅ olivaqu01 — written\n",
      "[278/569] ✅ reaveau01 — written\n",
      "[279/569] ✅ reddica01 — written\n",
      "[280/569] ✅ armeltr01 — written\n",
      "[281/569] ✅ vandeja01 — written\n",
      "[282/569] ✅ vincega01 — written\n",
      "[283/569] ✅ aldamsa01 — written\n",
      "[284/569] ✅ baglema01 — written\n",
      "[285/569] ✅ banede01 — written\n",
      "[286/569] ✅ castlco01 — written\n",
      "[287/569] ✅ clarkbr01 — written\n",
      "[288/569] ✅ edeyza01 — written\n",
      "[289/569] ✅ huffja01 — written\n",
      "[290/569] ✅ jacksgg01 — written\n",
      "[291/569] ✅ jacksja02 — written\n",
      "[292/569] ✅ kawamyu01 — written\n",
      "[293/569] ✅ kennalu01 — written\n",
      "[294/569] ✅ konchjo01 — written\n",
      "[295/569] ✅ laravja01 — written\n",
      "[296/569] ✅ moranja01 — written\n",
      "[297/569] ✅ pippesc02 — written\n",
      "[298/569] ✅ pullizy01 — written\n",
      "[299/569] ✅ smartma01 — written\n",
      "[300/569] ✅ spencca01 — written\n",
      "[301/569] ✅ stevela01 — written\n",
      "[302/569] ✅ wellsja01 — written\n",
      "[303/569] ✅ willivi01 — written\n",
      "[304/569] ✅ adebaba01 — written\n",
      "[305/569] ✅ burksal01 — written\n",
      "[306/569] ✅ chrisjo01 — written\n",
      "[307/569] ✅ herroty01 — written\n",
      "[308/569] ✅ highsha01 — written\n",
      "[309/569] ✅ jaqueja01 — written\n",
      "[310/569] ✅ johnske10 — written\n",
      "[311/569] ✅ jovicni01 — written\n",
      "[312/569] ✅ larsspe01 — written\n",
      "[313/569] ✅ loveke01 — written\n",
      "[314/569] ✅ mitchda01 — written\n",
      "[315/569] ✅ richajo01 — written\n",
      "[316/569] ✅ robindu01 — written\n",
      "[317/569] ✅ roziete01 — written\n",
      "[318/569] ✅ smithdr01 — written\n",
      "[319/569] ✅ steveis01 — written\n",
      "[320/569] ✅ wareke01 — written\n",
      "[321/569] ✅ antetgi01 — written\n",
      "[322/569] ✅ bouyeja01 — written\n",
      "[323/569] ✅ connapa01 — written\n",
      "[324/569] ✅ greenaj01 — written\n",
      "[325/569] ✅ jacksan01 — written\n",
      "[326/569] ✅ johnsaj01 — written\n",
      "[327/569] ✅ kuzmaky01 — written\n",
      "[328/569] ✅ lillada01 — written\n",
      "[329/569] ✅ livinch01 — written\n",
      "[330/569] ✅ lopezbr01 — written\n",
      "[331/569] ✅ middlkh01 — written\n",
      "[332/569] ✅ nancepe01 — written\n",
      "[333/569] ✅ portibo01 — written\n",
      "[334/569] ✅ princta02 — written\n",
      "[335/569] ✅ robbili01 — written\n",
      "[336/569] ✅ rolliry01 — written\n",
      "[337/569] ✅ simsje01 — written\n",
      "[338/569] ✅ smithty02 — written\n",
      "[339/569] ✅ trentga02 — written\n",
      "[340/569] ✅ umudest01 — written\n",
      "[341/569] ✅ wrighde01 — written\n",
      "[342/569] ✅ alexani01 — written\n",
      "[343/569] ✅ clarkja02 — written\n",
      "[344/569] ✅ conlemi01 — written\n",
      "[345/569] ✅ dilliro01 — written\n",
      "[346/569] ✅ divindo01 — written\n",
      "[347/569] ✅ doziepj01 — written\n",
      "[348/569] ✅ edwaran01 — written\n",
      "[349/569] ✅ edwarje01 — written\n",
      "[350/569] ✅ garzalu01 — written\n",
      "[351/569] ✅ goberru01 — written\n",
      "[352/569] ✅ inglejo01 — written\n",
      "[353/569] ✅ mcdanja02 — written\n",
      "[354/569] ✅ millele01 — written\n",
      "[355/569] ✅ minotjo01 — written\n",
      "[356/569] ✅ nixda01 — written\n",
      "[357/569] ✅ randlju01 — written\n",
      "[358/569] ✅ reidna01 — written\n",
      "[359/569] ✅ shannte01 — written\n",
      "[360/569] ✅ alvarjo01 — written\n",
      "[361/569] ✅ bostobr01 — written\n",
      "[362/569] ✅ brookke02 — written\n",
      "[363/569] ✅ brownbr01 — written\n",
      "[364/569] ✅ cainja01 — written\n",
      "[365/569] ✅ hawkijo01 — written\n",
      "[366/569] ✅ ingrabr01 — written\n",
      "[367/569] ✅ joneshe01 — written\n",
      "[368/569] ✅ matkoka01 — written\n",
      "[369/569] ✅ mccolcj01 — written\n",
      "[370/569] ✅ missiyv01 — written\n",
      "[371/569] ✅ murphtr02 — written\n",
      "[372/569] ✅ murrade01 — written\n",
      "[373/569] ✅ nowelja01 — written\n",
      "[374/569] ✅ olynyke01 — written\n",
      "[375/569] ✅ quinole01 — written\n",
      "[376/569] ✅ reevean01 — written\n",
      "[377/569] ✅ robinje02 — written\n",
      "[378/569] ✅ theisda01 — written\n",
      "[379/569] ✅ willizi01 — written\n",
      "[380/569] ✅ achiupr01 — written\n",
      "[381/569] ✅ anunoog01 — written\n",
      "[382/569] ✅ bridgmi01 — written\n",
      "[383/569] ✅ brunsja01 — written\n",
      "[384/569] ✅ dadiepa01 — written\n",
      "[385/569] ✅ hartjo01 — written\n",
      "[386/569] ✅ hukpoar01 — written\n",
      "[387/569] ✅ kolekty01 — written\n",
      "[388/569] ✅ mcbrimi01 — written\n",
      "[389/569] ✅ mcculke01 — written\n",
      "[390/569] ✅ payneca01 — written\n",
      "[391/569] ✅ robinmi01 — written\n",
      "[392/569] ✅ ryanma01 — written\n",
      "[393/569] ✅ shamela01 — written\n",
      "[394/569] ✅ townska01 — written\n",
      "[395/569] ✅ tuckepj01 — written\n",
      "[396/569] ✅ watsoan02 — written\n",
      "[397/569] ✅ carlsbr01 — written\n",
      "[398/569] ✅ carusal01 — written\n",
      "[399/569] ✅ diengou01 — written\n",
      "[400/569] ✅ dortlu01 — written\n",
      "[401/569] ✅ ducasal01 — written\n",
      "[402/569] ✅ flaglad01 — written\n",
      "[403/569] ✅ gilgesh01 — written\n",
      "[404/569] ✅ harteis01 — written\n",
      "[405/569] ✅ holmgch01 — written\n",
      "[406/569] ✅ joeis01 — written\n",
      "[407/569] ✅ jonesdi01 — written\n",
      "[408/569] ✅ leonsma01 — written\n",
      "[409/569] ✅ mitchaj01 — written\n",
      "[410/569] ✅ reeseal01 — written\n",
      "[411/569] ✅ wallaca01 — written\n",
      "[412/569] ✅ wiggiaa01 — written\n",
      "[413/569] ✅ willija06 — written\n",
      "[414/569] ✅ willija07 — written\n",
      "[415/569] ✅ willike04 — written\n",
      "[416/569] ✅ anthoco01 — written\n",
      "[417/569] ✅ banchpa01 — written\n",
      "[418/569] ✅ bitadgo01 — written\n",
      "[419/569] ✅ blackan01 — written\n",
      "[420/569] ✅ caldwke01 — written\n",
      "[421/569] ✅ cartewe01 — written\n",
      "[422/569] ✅ dasiltr01 — written\n",
      "[423/569] ✅ harriga01 — written\n",
      "[424/569] ✅ houstca01 — written\n",
      "[425/569] ✅ howarje01 — written\n",
      "[426/569] ✅ isaacjo01 — written\n",
      "[427/569] ✅ josepco01 — written\n",
      "[428/569] ✅ mccluma01 — written\n",
      "[429/569] ✅ queentr01 — written\n",
      "[430/569] ✅ suggsja01 — written\n",
      "[431/569] ✅ wagnefr01 — written\n",
      "[432/569] ✅ wagnemo01 — written\n",
      "[433/569] ✅ baglema02 — written\n",
      "[434/569] ✅ bonaad01 — written\n",
      "[435/569] ✅ brissos01 — written\n",
      "[436/569] ✅ butleja02 — written\n",
      "[437/569] ✅ councri01 — written\n",
      "[438/569] ✅ dowtije01 — written\n",
      "[439/569] ✅ drumman01 — written\n",
      "[440/569] ✅ edwarju01 — written\n",
      "[441/569] ✅ embiijo01 — written\n",
      "[442/569] ✅ georgpa01 — written\n",
      "[443/569] ✅ gordoer01 — written\n",
      "[444/569] ✅ jacksre01 — written\n",
      "[445/569] ✅ lowryky01 — written\n",
      "[446/569] ✅ martike04 — written\n",
      "[447/569] ✅ maxeyty01 — written\n",
      "[448/569] ✅ mccaija01 — written\n",
      "[449/569] ✅ mobleis01 — written\n",
      "[450/569] ✅ oubreke01 — written\n",
      "[451/569] ✅ walkelo01 — written\n",
      "[452/569] ✅ wheelph02 — written\n",
      "[453/569] ✅ yabusgu01 — written\n",
      "[454/569] ✅ allengr01 — written\n",
      "[455/569] ✅ bealbr01 — written\n",
      "[456/569] ✅ bolbo01 — written\n",
      "[457/569] ✅ bookede01 — written\n",
      "[458/569] ✅ bridgja01 — written\n",
      "[459/569] ✅ dunnry01 — written\n",
      "[460/569] ✅ duranke01 — written\n",
      "[461/569] ✅ gilleco01 — written\n",
      "[462/569] ✅ ighodos01 — written\n",
      "[463/569] ✅ jonesty01 — written\n",
      "[464/569] ✅ leeda03 — written\n",
      "[465/569] ✅ morrimo01 — written\n",
      "[466/569] ✅ onealro01 — written\n",
      "[467/569] ✅ plumlma01 — written\n",
      "[468/569] ✅ washity02 — written\n",
      "[469/569] ✅ avdijde01 — written\n",
      "[470/569] ✅ aytonde01 — written\n",
      "[471/569] ✅ bantoda01 — written\n",
      "[472/569] ✅ camarto01 — written\n",
      "[473/569] ✅ cissosi01 — written\n",
      "[474/569] ✅ clingdo01 — written\n",
      "[475/569] ✅ grantje01 — written\n",
      "[476/569] ✅ hendesc01 — written\n",
      "[477/569] ✅ mcgowbr01 — written\n",
      "[478/569] ✅ minayju01 — written\n",
      "[479/569] ✅ mooreta02 — written\n",
      "[480/569] ✅ murrakr01 — written\n",
      "[481/569] ✅ reathdu01 — written\n",
      "[482/569] ✅ ruperra01 — written\n",
      "[483/569] ✅ sharpsh01 — written\n",
      "[484/569] ✅ simonan01 — written\n",
      "[485/569] ✅ thybuma01 — written\n",
      "[486/569] ✅ walkeja01 — written\n",
      "[487/569] ✅ williro04 — written\n",
      "[488/569] ✅ cartede02 — written\n",
      "[489/569] ✅ crawfis01 — written\n",
      "[490/569] ✅ crowdja01 — written\n",
      "[491/569] 💥 Error for https://www.basketball-reference.com/players/d/daviste02.html: Failed to fetch https://www.basketball-reference.com/players/d/daviste02.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/d/daviste02.html\n",
      "[492/569] 💥 Error for https://www.basketball-reference.com/players/d/derozde01.html: Failed to fetch https://www.basketball-reference.com/players/d/derozde01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/d/derozde01.html\n",
      "[493/569] 💥 Error for https://www.basketball-reference.com/players/e/elliske01.html: Failed to fetch https://www.basketball-reference.com/players/e/elliske01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/e/elliske01.html\n",
      "[494/569] 💥 Error for https://www.basketball-reference.com/players/f/foxde01.html: Failed to fetch https://www.basketball-reference.com/players/f/foxde01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/f/foxde01.html\n",
      "[495/569] 💥 Error for https://www.basketball-reference.com/players/f/fultzma01.html: Failed to fetch https://www.basketball-reference.com/players/f/fultzma01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/f/fultzma01.html\n",
      "[496/569] 💥 Error for https://www.basketball-reference.com/players/j/jonesco02.html: Failed to fetch https://www.basketball-reference.com/players/j/jonesco02.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/j/jonesco02.html\n",
      "[497/569] 💥 Error for https://www.basketball-reference.com/players/j/jonesis01.html: Failed to fetch https://www.basketball-reference.com/players/j/jonesis01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/j/jonesis01.html\n",
      "[498/569] 💥 Error for https://www.basketball-reference.com/players/j/jonesma05.html: Failed to fetch https://www.basketball-reference.com/players/j/jonesma05.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/j/jonesma05.html\n",
      "[499/569] 💥 Error for https://www.basketball-reference.com/players/l/labissk01.html: Failed to fetch https://www.basketball-reference.com/players/l/labissk01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/l/labissk01.html\n",
      "[500/569] 💥 Error for https://www.basketball-reference.com/players/l/lylestr01.html: Failed to fetch https://www.basketball-reference.com/players/l/lylestr01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/l/lylestr01.html\n",
      "[501/569] 💥 Error for https://www.basketball-reference.com/players/m/mcderdo01.html: Failed to fetch https://www.basketball-reference.com/players/m/mcderdo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mcderdo01.html\n",
      "[502/569] 💥 Error for https://www.basketball-reference.com/players/m/mclaujo01.html: Failed to fetch https://www.basketball-reference.com/players/m/mclaujo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mclaujo01.html\n",
      "[503/569] 💥 Error for https://www.basketball-reference.com/players/m/monkma01.html: Failed to fetch https://www.basketball-reference.com/players/m/monkma01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/monkma01.html\n",
      "[504/569] 💥 Error for https://www.basketball-reference.com/players/m/murrake02.html: Failed to fetch https://www.basketball-reference.com/players/m/murrake02.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/murrake02.html\n",
      "[505/569] 💥 Error for https://www.basketball-reference.com/players/r/robinor01.html: Failed to fetch https://www.basketball-reference.com/players/r/robinor01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/r/robinor01.html\n",
      "[506/569] 💥 Error for https://www.basketball-reference.com/players/s/sabondo01.html: Failed to fetch https://www.basketball-reference.com/players/s/sabondo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sabondo01.html\n",
      "[507/569] 💥 Error for https://www.basketball-reference.com/players/t/taylote01.html: Failed to fetch https://www.basketball-reference.com/players/t/taylote01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/t/taylote01.html\n",
      "[508/569] 💥 Error for https://www.basketball-reference.com/players/v/valanjo01.html: Failed to fetch https://www.basketball-reference.com/players/v/valanjo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/v/valanjo01.html\n",
      "[509/569] 💥 Error for https://www.basketball-reference.com/players/b/barneha02.html: Failed to fetch https://www.basketball-reference.com/players/b/barneha02.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/barneha02.html\n",
      "[510/569] 💥 Error for https://www.basketball-reference.com/players/b/bassech01.html: Failed to fetch https://www.basketball-reference.com/players/b/bassech01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/bassech01.html\n",
      "[511/569] 💥 Error for https://www.basketball-reference.com/players/b/biyombi01.html: Failed to fetch https://www.basketball-reference.com/players/b/biyombi01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/biyombi01.html\n",
      "[512/569] 💥 Error for https://www.basketball-reference.com/players/b/branhma01.html: Failed to fetch https://www.basketball-reference.com/players/b/branhma01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/branhma01.html\n",
      "[513/569] 💥 Error for https://www.basketball-reference.com/players/c/castlst01.html: Failed to fetch https://www.basketball-reference.com/players/c/castlst01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/castlst01.html\n",
      "[514/569] 💥 Error for https://www.basketball-reference.com/players/c/champju02.html: Failed to fetch https://www.basketball-reference.com/players/c/champju02.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/champju02.html\n",
      "[515/569] 💥 Error for https://www.basketball-reference.com/players/d/dukeda01.html: Failed to fetch https://www.basketball-reference.com/players/d/dukeda01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/d/dukeda01.html\n",
      "[516/569] 💥 Error for https://www.basketball-reference.com/players/i/ingraha01.html: Failed to fetch https://www.basketball-reference.com/players/i/ingraha01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/i/ingraha01.html\n",
      "[517/569] 💥 Error for https://www.basketball-reference.com/players/j/johnske04.html: Failed to fetch https://www.basketball-reference.com/players/j/johnske04.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/j/johnske04.html\n",
      "[518/569] 💥 Error for https://www.basketball-reference.com/players/m/mamuksa01.html: Failed to fetch https://www.basketball-reference.com/players/m/mamuksa01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mamuksa01.html\n",
      "[519/569] 💥 Error for https://www.basketball-reference.com/players/m/minixri01.html: Failed to fetch https://www.basketball-reference.com/players/m/minixri01.html after 5 attempts. Last error: HTTPSConnectionPool(host='api.scraperapi.com', port=443): Read timed out. (read timeout=45)\n",
      "[520/569] 💥 Error for https://www.basketball-reference.com/players/p/paulch01.html: Failed to fetch https://www.basketball-reference.com/players/p/paulch01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/p/paulch01.html\n",
      "[521/569] 💥 Error for https://www.basketball-reference.com/players/s/sochaje01.html: Failed to fetch https://www.basketball-reference.com/players/s/sochaje01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sochaje01.html\n",
      "[522/569] 💥 Error for https://www.basketball-reference.com/players/v/vassede01.html: Failed to fetch https://www.basketball-reference.com/players/v/vassede01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/v/vassede01.html\n",
      "[523/569] 💥 Error for https://www.basketball-reference.com/players/w/wembavi01.html: Failed to fetch https://www.basketball-reference.com/players/w/wembavi01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/w/wembavi01.html\n",
      "[524/569] 💥 Error for https://www.basketball-reference.com/players/w/weslebl01.html: Failed to fetch https://www.basketball-reference.com/players/w/weslebl01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/w/weslebl01.html\n",
      "[525/569] 💥 Error for https://www.basketball-reference.com/players/a/agbajoc01.html: Failed to fetch https://www.basketball-reference.com/players/a/agbajoc01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/a/agbajoc01.html\n",
      "[526/569] 💥 Error for https://www.basketball-reference.com/players/b/barnesc01.html: Failed to fetch https://www.basketball-reference.com/players/b/barnesc01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/barnesc01.html\n",
      "[527/569] 💥 Error for https://www.basketball-reference.com/players/b/barrerj01.html: Failed to fetch https://www.basketball-reference.com/players/b/barrerj01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/barrerj01.html\n",
      "[528/569] 💥 Error for https://www.basketball-reference.com/players/b/battlja01.html: Failed to fetch https://www.basketball-reference.com/players/b/battlja01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/battlja01.html\n",
      "[529/569] 💥 Error for https://www.basketball-reference.com/players/b/bouchch01.html: Failed to fetch https://www.basketball-reference.com/players/b/bouchch01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/bouchch01.html\n",
      "[530/569] 💥 Error for https://www.basketball-reference.com/players/c/cartodj01.html: Failed to fetch https://www.basketball-reference.com/players/c/cartodj01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/cartodj01.html\n",
      "[531/569] 💥 Error for https://www.basketball-reference.com/players/c/chomcul01.html: Failed to fetch https://www.basketball-reference.com/players/c/chomcul01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/chomcul01.html\n",
      "[532/569] 💥 Error for https://www.basketball-reference.com/players/d/dickgr01.html: Failed to fetch https://www.basketball-reference.com/players/d/dickgr01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/d/dickgr01.html\n",
      "[533/569] 💥 Error for https://www.basketball-reference.com/players/f/fernabr01.html: Failed to fetch https://www.basketball-reference.com/players/f/fernabr01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/f/fernabr01.html\n",
      "[534/569] 💥 Error for https://www.basketball-reference.com/players/l/lawsoaj01.html: Failed to fetch https://www.basketball-reference.com/players/l/lawsoaj01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/l/lawsoaj01.html\n",
      "[535/569] 💥 Error for https://www.basketball-reference.com/players/m/mogbojo01.html: Failed to fetch https://www.basketball-reference.com/players/m/mogbojo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mogbojo01.html\n",
      "[536/569] 💥 Error for https://www.basketball-reference.com/players/p/poeltja01.html: Failed to fetch https://www.basketball-reference.com/players/p/poeltja01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/p/poeltja01.html\n",
      "[537/569] 💥 Error for https://www.basketball-reference.com/players/q/quickim01.html: Failed to fetch https://www.basketball-reference.com/players/q/quickim01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/q/quickim01.html\n",
      "[538/569] 💥 Error for https://www.basketball-reference.com/players/s/sheadja01.html: Failed to fetch https://www.basketball-reference.com/players/s/sheadja01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sheadja01.html\n",
      "[539/569] 💥 Error for https://www.basketball-reference.com/players/t/templga01.html: Failed to fetch https://www.basketball-reference.com/players/t/templga01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/t/templga01.html\n",
      "[540/569] 💥 Error for https://www.basketball-reference.com/players/w/walteja01.html: Failed to fetch https://www.basketball-reference.com/players/w/walteja01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/w/walteja01.html\n",
      "[541/569] 💥 Error for https://www.basketball-reference.com/players/c/clarkjo01.html: Failed to fetch https://www.basketball-reference.com/players/c/clarkjo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/clarkjo01.html\n",
      "[542/569] 💥 Error for https://www.basketball-reference.com/players/c/colliis01.html: Failed to fetch https://www.basketball-reference.com/players/c/colliis01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/colliis01.html\n",
      "[543/569] 💥 Error for https://www.basketball-reference.com/players/c/collijo01.html: Failed to fetch https://www.basketball-reference.com/players/c/collijo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/collijo01.html\n",
      "[544/569] 💥 Error for https://www.basketball-reference.com/players/f/filipky01.html: Failed to fetch https://www.basketball-reference.com/players/f/filipky01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/f/filipky01.html\n",
      "[545/569] 💥 Error for https://www.basketball-reference.com/players/g/georgke01.html: Failed to fetch https://www.basketball-reference.com/players/g/georgke01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/g/georgke01.html\n",
      "[546/569] 💥 Error for https://www.basketball-reference.com/players/h/harklej01.html: Failed to fetch https://www.basketball-reference.com/players/h/harklej01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/h/harklej01.html\n",
      "[547/569] 💥 Error for https://www.basketball-reference.com/players/h/hendrta01.html: Failed to fetch https://www.basketball-reference.com/players/h/hendrta01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/h/hendrta01.html\n",
      "[548/569] 💥 Error for https://www.basketball-reference.com/players/j/juzanjo01.html: Failed to fetch https://www.basketball-reference.com/players/j/juzanjo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/j/juzanjo01.html\n",
      "[549/569] 💥 Error for https://www.basketball-reference.com/players/k/kesslwa01.html: Failed to fetch https://www.basketball-reference.com/players/k/kesslwa01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/k/kesslwa01.html\n",
      "[550/569] 💥 Error for https://www.basketball-reference.com/players/m/markkla01.html: Failed to fetch https://www.basketball-reference.com/players/m/markkla01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/markkla01.html\n",
      "[551/569] 💥 Error for https://www.basketball-reference.com/players/m/mykhasv01.html: Failed to fetch https://www.basketball-reference.com/players/m/mykhasv01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mykhasv01.html\n",
      "[552/569] 💥 Error for https://www.basketball-reference.com/players/p/pottemi01.html: Failed to fetch https://www.basketball-reference.com/players/p/pottemi01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/p/pottemi01.html\n",
      "[553/569] 💥 Error for https://www.basketball-reference.com/players/s/sensabr01.html: Failed to fetch https://www.basketball-reference.com/players/s/sensabr01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sensabr01.html\n",
      "[554/569] 💥 Error for https://www.basketball-reference.com/players/s/sextoco01.html: Failed to fetch https://www.basketball-reference.com/players/s/sextoco01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sextoco01.html\n",
      "[555/569] 💥 Error for https://www.basketball-reference.com/players/t/tshieos01.html: Failed to fetch https://www.basketball-reference.com/players/t/tshieos01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/t/tshieos01.html\n",
      "[556/569] 💥 Error for https://www.basketball-reference.com/players/w/willico04.html: Failed to fetch https://www.basketball-reference.com/players/w/willico04.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/w/willico04.html\n",
      "[557/569] 💥 Error for https://www.basketball-reference.com/players/b/brogdma01.html: Failed to fetch https://www.basketball-reference.com/players/b/brogdma01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/b/brogdma01.html\n",
      "[558/569] 💥 Error for https://www.basketball-reference.com/players/c/carrica01.html: Failed to fetch https://www.basketball-reference.com/players/c/carrica01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/carrica01.html\n",
      "[559/569] 💥 Error for https://www.basketball-reference.com/players/c/champju01.html: Failed to fetch https://www.basketball-reference.com/players/c/champju01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/champju01.html\n",
      "[560/569] 💥 Error for https://www.basketball-reference.com/players/c/coulibi01.html: Failed to fetch https://www.basketball-reference.com/players/c/coulibi01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/c/coulibi01.html\n",
      "[561/569] 💥 Error for https://www.basketball-reference.com/players/d/davisjo06.html: Failed to fetch https://www.basketball-reference.com/players/d/davisjo06.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/d/davisjo06.html\n",
      "[562/569] 💥 Error for https://www.basketball-reference.com/players/g/georgky01.html: Failed to fetch https://www.basketball-reference.com/players/g/georgky01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/g/georgky01.html\n",
      "[563/569] 💥 Error for https://www.basketball-reference.com/players/g/gillan01.html: Failed to fetch https://www.basketball-reference.com/players/g/gillan01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/g/gillan01.html\n",
      "[564/569] 💥 Error for https://www.basketball-reference.com/players/h/holmeri01.html: Failed to fetch https://www.basketball-reference.com/players/h/holmeri01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/h/holmeri01.html\n",
      "[565/569] 💥 Error for https://www.basketball-reference.com/players/k/kispeco01.html: Failed to fetch https://www.basketball-reference.com/players/k/kispeco01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/k/kispeco01.html\n",
      "[566/569] 💥 Error for https://www.basketball-reference.com/players/m/mcdanja01.html: Failed to fetch https://www.basketball-reference.com/players/m/mcdanja01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/m/mcdanja01.html\n",
      "[567/569] 💥 Error for https://www.basketball-reference.com/players/p/poolejo01.html: Failed to fetch https://www.basketball-reference.com/players/p/poolejo01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/p/poolejo01.html\n",
      "[568/569] 💥 Error for https://www.basketball-reference.com/players/s/sarral01.html: Failed to fetch https://www.basketball-reference.com/players/s/sarral01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/s/sarral01.html\n",
      "[569/569] 💥 Error for https://www.basketball-reference.com/players/v/vukcetr01.html: Failed to fetch https://www.basketball-reference.com/players/v/vukcetr01.html after 5 attempts. Last error: HTTP 403 for https://www.basketball-reference.com/players/v/vukcetr01.html\n",
      "📝 Finished. CSV written to output_data/all_nba_players_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Networking + DOM helpers\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def fetch_html_via_scraperapi(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    retries: int = 5,\n",
    "    timeout: int = 45,\n",
    "    jitter_range = (0.2, 0.7),\n",
    "    base: str = \"https://api.scraperapi.com\",\n",
    "    user_agent: str = \"Mozilla/5.0 (compatible; BBRBot/1.0)\"\n",
    ") -> str:\n",
    "    key = \"aaa492ea5514911b40ac2e7679e21da7\"  # hardcoded per your request\n",
    "    if not key:\n",
    "        raise RuntimeError(\"SCRAPERAPI_KEY not set. export SCRAPERAPI_KEY='your_key' or pass api_key.\")\n",
    "    params = {\"api_key\": key, \"url\": url}\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            r = requests.get(base, params=params, headers=headers, timeout=timeout)\n",
    "            if r.status_code == 200 and r.text:\n",
    "                return r.text\n",
    "            last_err = RuntimeError(f\"HTTP {r.status_code} for {url}\")\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e\n",
    "        time.sleep(min(1.5 ** attempt, 12) + random.uniform(*jitter_range))\n",
    "    raise RuntimeError(f\"Failed to fetch {url} after {retries} attempts. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "def _build_dom(html_text: str) -> html.HtmlElement:\n",
    "    \"\"\"\n",
    "    Build an lxml DOM and also append any tables that may be hidden inside HTML comments.\n",
    "    Basketball-Reference sometimes wraps tables in comments (e.g., per_game, per_minute, per_poss, advanced, contracts_*).\n",
    "    \"\"\"\n",
    "    # Use substrings (not exact ids) so contracts_* (contracts_lac, contracts_pho...) get included.\n",
    "    TABLE_ID_SUBSTRINGS = (\n",
    "        \"per_game_stats\",\n",
    "        \"per_minute_stats\",\n",
    "        \"per_poss\",\n",
    "        \"advanced\",\n",
    "        \"all_salaries\",\n",
    "        \"contracts_\",   # <-- match any contracts_* table\n",
    "    )\n",
    "\n",
    "    dom = html.fromstring(html_text)\n",
    "\n",
    "    for c in dom.xpath('//comment()'):\n",
    "        c_text = c.text or \"\"\n",
    "        if \"<table\" not in c_text:\n",
    "            continue\n",
    "        # Only unhide if the comment contains one of our desired table id substrings\n",
    "        if any(sub in c_text for sub in TABLE_ID_SUBSTRINGS):\n",
    "            try:\n",
    "                sub = html.fromstring(c_text)\n",
    "                for t in sub.xpath(\".//table\"):\n",
    "                    tid = t.get(\"id\") or \"\"\n",
    "                    # Append if the id contains any of our substrings\n",
    "                    if any(sub in tid for sub in TABLE_ID_SUBSTRINGS):\n",
    "                        dom.append(t)\n",
    "            except Exception:\n",
    "                # Ignore parsing errors in odd comment blocks\n",
    "                pass\n",
    "\n",
    "    return dom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Personal info (player_id logic inlined here)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def players_personal_info(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Fetches HTML if needed and parses personal info.\n",
    "    player_id is derived inline from the URL.\n",
    "    \"\"\"\n",
    "    # inline player_id extraction\n",
    "    m = re.search(r\"/([^/]+)\\.html?$\", url)\n",
    "    player_id = m.group(1).lower() if m else None\n",
    "\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    # player_name\n",
    "    player_name = ''.join(dom.xpath(\"//div[@id='meta']//h1//span/text()\")).strip()\n",
    "    if not player_name:\n",
    "        og = dom.xpath(\"//meta[@property='og:title']/@content\")\n",
    "        if og:\n",
    "            player_name = og[0].split(\" Stats\", 1)[0].strip()\n",
    "\n",
    "    # team\n",
    "    team = ''.join(dom.xpath(\"//div[@id='meta']//p[strong[normalize-space()='Team']]/a/text()\")).strip()\n",
    "    if not team:\n",
    "        p_text = dom.xpath(\"normalize-space(//div[@id='meta']//p[strong[normalize-space()='Team']])\")\n",
    "        if p_text and \":\" in p_text:\n",
    "            team = p_text.split(\":\", 1)[1].strip()\n",
    "\n",
    "    # birth_day\n",
    "    birth_day = ''.join(dom.xpath(\"//strong[normalize-space()='Born:']/following-sibling::span[@id='necro-birth']/@data-birth\")).strip()\n",
    "    if not birth_day:\n",
    "        birth_day = ''.join(dom.xpath(\"//strong[contains(.,'Born')]/following-sibling::span[@id='necro-birth']/@data-birth\")).strip()\n",
    "\n",
    "    # years_experience\n",
    "    exp_line = dom.xpath(\"string(//div[@id='meta']//p[strong[normalize-space()='Experience:']])\")\n",
    "    years_experience = None\n",
    "    if exp_line:\n",
    "        m_exp = re.search(r\"Experience:\\s*(\\d+)\", exp_line, flags=re.IGNORECASE)\n",
    "        if m_exp:\n",
    "            try:\n",
    "                years_experience = int(m_exp.group(1))\n",
    "            except ValueError:\n",
    "                years_experience = None\n",
    "\n",
    "    return {\n",
    "        \"player_id\": player_id,\n",
    "        \"player_name\": player_name or None,\n",
    "        \"team\": team or None,\n",
    "        \"birth_day\": birth_day or None,\n",
    "        \"years_experience\": years_experience\n",
    "    }\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Per Game\n",
    "#  - rename year_id -> season\n",
    "#  - no 'per_g' on specific fields; rename mapping below\n",
    "#  - keep 'age' as 'age'\n",
    "#  - all other fields get '_per_g' appended if not present\n",
    "#  - OUTPUT ORDER: strictly left→right by table header\n",
    "#  - NEW: was_traded, teams_count, teams_played (abbr list, comma-separated)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def per_game_stats(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    prefer_tot: bool = True,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    table_nodes = dom.xpath(\"//table[@id='per_game_stats']\")\n",
    "    if not table_nodes:\n",
    "        return None\n",
    "    table = table_nodes[0]\n",
    "\n",
    "    # Build header order (left→right)\n",
    "    header_stats: List[str] = []\n",
    "    for th in table.xpath(\".//thead//tr[1]/th\"):\n",
    "        ds = th.get(\"data-stat\")\n",
    "        if ds:\n",
    "            header_stats.append(ds)\n",
    "\n",
    "    # ---------- current season (tbody) ----------\n",
    "    rows = table.xpath(\".//tbody/tr[th[@data-stat='year_id']]\")\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    seasons: List[tuple[int, Any]] = []\n",
    "    for tr in rows:\n",
    "        th = tr.xpath(\"./th[@data-stat='year_id']\")[0]\n",
    "        csk = th.get(\"csk\")\n",
    "        try:\n",
    "            csk_int = int(csk)\n",
    "        except (TypeError, ValueError):\n",
    "            txt = th.xpath(\"normalize-space(string(.))\")\n",
    "            m = re.search(r\"(\\d{4})(?:-\\d{2})?$\", txt or \"\")\n",
    "            csk_int = int(m.group(1)) if m else 0\n",
    "        seasons.append((csk_int, tr))\n",
    "\n",
    "    max_csk = max(c for c, _ in seasons)\n",
    "    latest_rows = [tr for (c, tr) in seasons if c == max_csk]\n",
    "\n",
    "    # Gather all team abbreviations for that season (exclude TOT), preserve order, de-dup\n",
    "    teams_abbrs: List[str] = []\n",
    "    seen_team = set()\n",
    "    for tr in latest_rows:\n",
    "        abbr = ''.join(tr.xpath(\"./td[@data-stat='team_name_abbr']//text()\")).strip()\n",
    "        if not abbr or abbr.upper() == \"TOT\":\n",
    "            continue\n",
    "        if abbr not in seen_team:\n",
    "            seen_team.add(abbr)\n",
    "            teams_abbrs.append(abbr)\n",
    "\n",
    "    was_traded = \"Y\" if len(teams_abbrs) > 1 else \"N\"\n",
    "    teams_count = 1 if len(teams_abbrs) == 1 else len(teams_abbrs) - 1\n",
    "    teams_played = \",\".join(teams_abbrs)\n",
    "\n",
    "    # Prefer TOT if traded for the actual stat line chosen\n",
    "    if len(latest_rows) == 1 or not prefer_tot:\n",
    "        chosen = latest_rows[0]\n",
    "    else:\n",
    "        chosen = None\n",
    "        for tr in latest_rows:\n",
    "            team_txt = ''.join(tr.xpath(\"./td[@data-stat='team_name_abbr']//text()\")).strip()\n",
    "            if team_txt.upper() == \"TOT\":\n",
    "                chosen = tr\n",
    "                break\n",
    "        if chosen is None:\n",
    "            chosen = latest_rows[0]\n",
    "\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # season comes from th[data-stat=year_id]\n",
    "    th = chosen.xpath(\"./th[@data-stat='year_id']\")[0]\n",
    "    out[\"season\"] = th.xpath(\"normalize-space(string(.))\")  # e.g., \"2024-25\"\n",
    "\n",
    "    # Lookup of row cells by data-stat\n",
    "    td_by_stat = {td.get(\"data-stat\"): td for td in chosen.xpath(\"./td[@data-stat]\")}\n",
    "\n",
    "    # Fields that should NOT get \"_per_g\" and/or need renaming\n",
    "    no_per_g_and_rename = {\n",
    "        \"team_name_abbr\": \"team_id\",\n",
    "        \"comp_name_abbr\": \"lg_id\",\n",
    "        \"pos\": \"pos\",\n",
    "        \"games\": \"g\",\n",
    "        \"games_started\": \"gs\",\n",
    "        \"awards\": \"awards\",\n",
    "    }\n",
    "\n",
    "    # Iterate columns in the header’s left→right order for current season\n",
    "    for stat in header_stats:\n",
    "        if stat == \"year_id\":\n",
    "            continue  # already 'season'\n",
    "\n",
    "        td = td_by_stat.get(stat)\n",
    "        if td is None:\n",
    "            continue\n",
    "        val = td.xpath(\"normalize-space(string(.))\")\n",
    "\n",
    "        if stat == \"age\":\n",
    "            out[\"age\"] = val\n",
    "            continue\n",
    "\n",
    "        if stat in no_per_g_and_rename:\n",
    "            out[no_per_g_and_rename[stat]] = val\n",
    "        else:\n",
    "            key = stat if stat.endswith(\"_per_g\") else f\"{stat}_per_g\"\n",
    "            out[key] = val\n",
    "\n",
    "    # Append trade metadata fields\n",
    "    out[\"was_traded\"] = was_traded\n",
    "    out[\"teams_count\"] = teams_count\n",
    "    out[\"teams_played\"] = teams_played\n",
    "\n",
    "    # ---------- career row (tfoot) ----------\n",
    "    # We want the overall career (e.g., \"3 Yrs\"), not franchise-specific summaries.\n",
    "    tfoot = table.xpath(\"./tfoot\")\n",
    "    if tfoot:\n",
    "        tf = tfoot[0]\n",
    "        candidates = tf.xpath(\".//tr[th[@data-stat='year_id'] and .//td[@data-stat]]\")\n",
    "        career_tr = None\n",
    "        if candidates:\n",
    "            # Choose the row with the largest number of seasons (\"X Yrs\") or widest colspan.\n",
    "            best = None\n",
    "            best_score = -1\n",
    "            for tr in candidates:\n",
    "                th = tr.xpath(\"./th[@data-stat='year_id']\")[0]\n",
    "                # Score #1: parse \"X Yrs\" text\n",
    "                txt = th.xpath(\"normalize-space(string(.))\")\n",
    "                m_yrs = re.search(r\"(\\d+)\\s+Yrs\", txt)\n",
    "                yrs_val = int(m_yrs.group(1)) if m_yrs else 0\n",
    "                # Score #2: colspan fallback\n",
    "                colspan = th.get(\"colspan\")\n",
    "                try:\n",
    "                    colspan_val = int(colspan)\n",
    "                except (TypeError, ValueError):\n",
    "                    colspan_val = 0\n",
    "                score = max(yrs_val, colspan_val)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best = tr\n",
    "            career_tr = best\n",
    "\n",
    "        if career_tr is not None:\n",
    "            # Build a lookup of TDs by data-stat for the chosen career row\n",
    "            c_td_by_stat = {td.get(\"data-stat\"): td for td in career_tr.xpath(\"./td[@data-stat]\")}\n",
    "\n",
    "            # Exclude fields that don't make sense on career summary\n",
    "            career_exclude = {\n",
    "                \"year_id\", \"team_name_abbr\", \"comp_name_abbr\", \"pos\", \"awards\"\n",
    "            }\n",
    "\n",
    "            for stat in header_stats:\n",
    "                if stat in career_exclude:\n",
    "                    continue\n",
    "                td = c_td_by_stat.get(stat)\n",
    "                if td is None:\n",
    "                    continue\n",
    "                val = td.xpath(\"normalize-space(string(.))\")\n",
    "\n",
    "                # Prefix every kept stat with 'career_'\n",
    "                out[f\"career_{stat}\"] = val\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "#Parse the 'Per 36 Minutes' table (id='per_minute_stats') and return the latest season row.\n",
    " # - Always picks the current season (max csk), preferring TOT if traded (when prefer_tot=True).\n",
    " # - Excludes fields: season, age, team_id, lg_id, pos, g, gs, awards, and trade metadata.\n",
    " # - For any data-stat that doesn't already include 'per_minute_36', append '_per_minute_36' to the output key.\n",
    " # - Preserves header left→right order.\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def per_36_min_stats(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    prefer_tot: bool = True,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    # ── fetch & DOM ───────────────────────────────────────────────────────────\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    tables = dom.xpath(\"//table[@id='per_minute_stats']\")\n",
    "    if not tables:\n",
    "        return None\n",
    "    table = tables[0]\n",
    "\n",
    "    # ── header (left→right) ──────────────────────────────────────────────────\n",
    "    header_stats: List[str] = []\n",
    "    ths = table.xpath(\".//thead//tr[1]/th\")\n",
    "    for th in ths:\n",
    "        ds = th.get(\"data-stat\")\n",
    "        if ds:\n",
    "            header_stats.append(ds)\n",
    "\n",
    "    # ── body rows with a season cell ──────────────────────────────────────────\n",
    "    rows = table.xpath(\".//tbody/tr[th[@data-stat='year_id']]\")\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    # Build (csk_int, tr)\n",
    "    seasons: List[tuple[int, Any]] = []\n",
    "    for tr in rows:\n",
    "        th = tr.xpath(\"./th[@data-stat='year_id']\")\n",
    "        if not th:\n",
    "            continue\n",
    "        csk = th[0].get(\"csk\")\n",
    "        csk_int = 0\n",
    "        if csk is not None:\n",
    "            try:\n",
    "                csk_int = int(csk)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if csk_int == 0:\n",
    "            txt = th[0].xpath(\"normalize-space(string(.))\")\n",
    "            m = re.search(r\"(\\d{4})(?:-\\d{2})?$\", txt or \"\")\n",
    "            if m:\n",
    "                try:\n",
    "                    csk_int = int(m.group(1))\n",
    "                except ValueError:\n",
    "                    csk_int = 0\n",
    "        seasons.append((csk_int, tr))\n",
    "\n",
    "    if not seasons:\n",
    "        return None\n",
    "\n",
    "    max_csk = max(c for c, _ in seasons)\n",
    "    latest_rows = [tr for (c, tr) in seasons if c == max_csk]\n",
    "\n",
    "    # ── config for selection/aggregation ──────────────────────────────────────\n",
    "    aggregate_markers = {\"TOT\", \"2TM\", \"3TM\", \"4TM\"}\n",
    "    exclude_stats = {\n",
    "        \"year_id\", \"age\", \"team_name_abbr\", \"comp_name_abbr\",\n",
    "        \"pos\", \"awards\"\n",
    "    }\n",
    "\n",
    "    # ── pick chosen row if an aggregate one exists (TOT/2TM/3TM/4TM) ─────────\n",
    "    chosen = None\n",
    "    if prefer_tot and len(latest_rows) > 1:\n",
    "        for tr in latest_rows:\n",
    "            # read team_name_abbr text\n",
    "            tds = tr.xpath(\"./td[@data-stat='team_name_abbr']\")\n",
    "            team_txt = tds[0].xpath(\"normalize-space(string(.))\").upper() if tds else \"\"\n",
    "            if team_txt in aggregate_markers:\n",
    "                chosen = tr\n",
    "                break\n",
    "\n",
    "    need_manual_aggregate = (chosen is None and len(latest_rows) > 1)\n",
    "\n",
    "    # ── output dict for current season ────────────────────────────────────────\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # Name mapping rule inline\n",
    "    # (append '_per_minute_36' if not already present)\n",
    "    # We'll implement inline each time we set a key.\n",
    "\n",
    "    # ── simple read path (have a chosen row or only one row) ─────────────────\n",
    "    if not need_manual_aggregate:\n",
    "        if chosen is None:\n",
    "            chosen = latest_rows[0]\n",
    "        # build map\n",
    "        tds = chosen.xpath(\"./td[@data-stat]\")\n",
    "        td_by_stat = {td.get(\"data-stat\"): td for td in tds}\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = td_by_stat.get(stat)\n",
    "            if td is None:\n",
    "                continue\n",
    "            val = td.xpath(\"normalize-space(string(.))\")\n",
    "            key = stat if \"per_minute_36\" in stat else f\"{stat}_per_minute_36\"\n",
    "            out[key] = val\n",
    "\n",
    "    # ── manual aggregation path (minutes-weighted) ────────────────────────────\n",
    "    else:\n",
    "        # helper inline parsing: convert a text to float safely\n",
    "        def _to_float_inline(s: str) -> Optional[float]:\n",
    "            s = (s or \"\").strip()\n",
    "            if not s:\n",
    "                return None\n",
    "            if s.startswith(\".\") and s[1:].replace(\".\", \"\", 1).isdigit():\n",
    "                s = \"0\" + s\n",
    "            try:\n",
    "                return float(s)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # filter out aggregate rows; keep per-team rows for the season\n",
    "        team_rows = []\n",
    "        for tr in latest_rows:\n",
    "            tds = tr.xpath(\"./td[@data-stat='team_name_abbr']\")\n",
    "            team_txt = tds[0].xpath(\"normalize-space(string(.))\").upper() if tds else \"\"\n",
    "            if team_txt not in aggregate_markers:\n",
    "                team_rows.append(tr)\n",
    "\n",
    "        # parse rows into numeric dicts + mp weights\n",
    "        parsed_rows: List[Dict[str, Optional[float]]] = []\n",
    "        for tr in team_rows:\n",
    "            row_dict: Dict[str, Optional[float]] = {}\n",
    "            # minutes weight from this table's MP\n",
    "            mp_td = tr.xpath(\"./td[@data-stat='mp']\")\n",
    "            mp_val = mp_td[0].xpath(\"normalize-space(string(.))\") if mp_td else \"\"\n",
    "            row_dict[\"mp\"] = _to_float_inline(mp_val) or 0.0\n",
    "            # parse candidates\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                td = tr.xpath(f\"./td[@data-stat='{stat}']\")\n",
    "                sval = td[0].xpath(\"normalize-space(string(.))\") if td else \"\"\n",
    "                row_dict[stat] = _to_float_inline(sval)\n",
    "            parsed_rows.append(row_dict)\n",
    "\n",
    "        total_mp = sum((r.get(\"mp\", 0.0) or 0.0) for r in parsed_rows)\n",
    "        if total_mp <= 0:\n",
    "            # fallback: use the first team row's strings\n",
    "            chosen = team_rows[0]\n",
    "            tds = chosen.xpath(\"./td[@data-stat]\")\n",
    "            td_by_stat = {td.get(\"data-stat\"): td for td in tds}\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                td = td_by_stat.get(stat)\n",
    "                if td is None:\n",
    "                    continue\n",
    "                val = td.xpath(\"normalize-space(string(.))\")\n",
    "                key = stat if \"per_minute_36\" in stat else f\"{stat}_per_minute_36\"\n",
    "                out[key] = val\n",
    "        else:\n",
    "            # weighted average by MP\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                num = 0.0\n",
    "                den = 0.0\n",
    "                for r in parsed_rows:\n",
    "                    v = r.get(stat)\n",
    "                    w = r.get(\"mp\", 0.0) or 0.0\n",
    "                    if v is None:\n",
    "                        continue\n",
    "                    num += v * w\n",
    "                    den += w\n",
    "                sval = \"\"\n",
    "                if den > 0:\n",
    "                    agg = num / den\n",
    "                    sval = f\"{agg:.3f}\".rstrip(\"0\").rstrip(\".\")\n",
    "                key = stat if \"per_minute_36\" in stat else f\"{stat}_per_minute_36\"\n",
    "                out[key] = sval\n",
    "\n",
    "    # ── career row from <tfoot> (robust) ──────────────────────────────────────\n",
    "    # Strategy:\n",
    "    #   1) Try to find a <tr> whose TH (data-stat=\"year_id\") is exactly like \"^\\d+ Yrs$\"\n",
    "    #   2) Else, among all tfoot rows that *contain* \"<number> Yrs\", pick the largest number\n",
    "    tf_rows = table.xpath(\".//tfoot/tr\")\n",
    "    career_tr = None\n",
    "    if tf_rows:\n",
    "        # pass 1: exact match\n",
    "        for tr in tf_rows:\n",
    "            th_text = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "            if re.fullmatch(r\"\\d+\\s+Yr(?:s)?\", th_text or \"\"):\n",
    "                career_tr = tr\n",
    "                break\n",
    "        # pass 2: best (max) \"<n> Yrs\"\n",
    "        if career_tr is None:\n",
    "            best = None\n",
    "            best_years = -1\n",
    "            for tr in tf_rows:\n",
    "                th_text = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "                m = re.search(r\"(\\d+)\\s+Yr(?:s)?\", th_text or \"\")\n",
    "                if m:\n",
    "                    yrs = int(m.group(1))\n",
    "                    if yrs > best_years:\n",
    "                        best_years = yrs\n",
    "                        best = tr\n",
    "            career_tr = best\n",
    "\n",
    "    if career_tr is not None:\n",
    "        td_by_stat_c = {td.get(\"data-stat\"): td for td in career_tr.xpath(\"./td[@data-stat]\")}\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = td_by_stat_c.get(stat)\n",
    "            if td is None:\n",
    "                continue\n",
    "            val = td.xpath(\"normalize-space(string(.))\")\n",
    "            key = stat if \"per_minute_36\" in stat else f\"{stat}_per_minute_36\"\n",
    "            out[f\"career_{key}\"] = val\n",
    "\n",
    "    return out\n",
    "\n",
    "       \n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "    #Parse the 'Per 100 Possessions' table (id='per_poss') and return the latest-season row.\n",
    "    #- Picks the current season (max csk). If multiple rows (traded), prefers the aggregate row\n",
    "    #  (TOT / 2TM / 3TM / 4TM) when prefer_tot=True.\n",
    "    #- EXCLUDES: season/year_id, age, team_id, lg_id, pos, g, gs, awards.\n",
    "    #- For any data-stat that doesn't already end with '_per_poss', append '_per_poss' to the key.\n",
    "    #- Preserves header left→right order.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def per_100_poss_stats(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    prefer_tot: bool = True,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    # ── fetch & DOM ───────────────────────────────────────────────────────────\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    # Primary id is 'per_poss'; keep 'per_poss_stats' as rare fallback\n",
    "    tables = dom.xpath(\"//table[@id='per_poss']\") or dom.xpath(\"//table[@id='per_poss_stats']\")\n",
    "    if not tables:\n",
    "        return None\n",
    "    table = tables[0]\n",
    "\n",
    "    # ── header (left→right) ──────────────────────────────────────────────────\n",
    "    header_stats: List[str] = []\n",
    "    ths = table.xpath(\".//thead//tr[1]/th\")\n",
    "    for th in ths:\n",
    "        ds = th.get(\"data-stat\")\n",
    "        if ds:\n",
    "            header_stats.append(ds)\n",
    "\n",
    "    # ── body rows with a season cell ──────────────────────────────────────────\n",
    "    rows = table.xpath(\".//tbody/tr[th[@data-stat='year_id']]\")\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    # Build (csk_int, tr)\n",
    "    seasons: List[tuple[int, Any]] = []\n",
    "    for tr in rows:\n",
    "        th = tr.xpath(\"./th[@data-stat='year_id']\")\n",
    "        if not th:\n",
    "            continue\n",
    "        csk = th[0].get(\"csk\")\n",
    "        csk_int = 0\n",
    "        if csk is not None:\n",
    "            try:\n",
    "                csk_int = int(csk)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if csk_int == 0:\n",
    "            txt = th[0].xpath(\"normalize-space(string(.))\")\n",
    "            m = re.search(r\"(\\d{4})(?:-\\d{2})?$\", txt or \"\")\n",
    "            if m:\n",
    "                try:\n",
    "                    csk_int = int(m.group(1))\n",
    "                except ValueError:\n",
    "                    csk_int = 0\n",
    "        seasons.append((csk_int, tr))\n",
    "\n",
    "    if not seasons:\n",
    "        return None\n",
    "\n",
    "    max_csk = max(c for c, _ in seasons)\n",
    "    latest_rows = [tr for (c, tr) in seasons if c == max_csk]\n",
    "\n",
    "    # ── config ────────────────────────────────────────────────────────────────\n",
    "    aggregate_markers = {\"TOT\", \"2TM\", \"3TM\", \"4TM\"}\n",
    "    exclude_stats = {\n",
    "        \"year_id\", \"age\", \"team_name_abbr\", \"comp_name_abbr\",\n",
    "        \"pos\", \"awards\"\n",
    "    }\n",
    "\n",
    "    # ── prefer official aggregate row (TOT/2TM/3TM/4TM) if present ───────────\n",
    "    chosen = None\n",
    "    if prefer_tot and len(latest_rows) > 1:\n",
    "        for tr in latest_rows:\n",
    "            tds = tr.xpath(\"./td[@data-stat='team_name_abbr']\")\n",
    "            team_txt = tds[0].xpath(\"normalize-space(string(.))\").upper() if tds else \"\"\n",
    "            if team_txt in aggregate_markers:\n",
    "                chosen = tr\n",
    "                break\n",
    "\n",
    "    need_manual_aggregate = (chosen is None and len(latest_rows) > 1)\n",
    "\n",
    "    # ── output dict for current season ────────────────────────────────────────\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # ── simple read path ─────────────────────────────────────────────────────\n",
    "    if not need_manual_aggregate:\n",
    "        if chosen is None:\n",
    "            chosen = latest_rows[0]\n",
    "        td_by_stat = {td.get(\"data-stat\"): td for td in chosen.xpath(\"./td[@data-stat]\")}\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = td_by_stat.get(stat)\n",
    "            if td is None:\n",
    "                continue\n",
    "            val = td.xpath(\"normalize-space(string(.))\")\n",
    "            key = stat if stat.endswith(\"_per_poss\") else f\"{stat}_per_poss\"\n",
    "            out[key] = val\n",
    "    else:\n",
    "        # ── manual aggregation (minutes-weighted) ────────────────────────────\n",
    "        def _to_float_inline(s: str) -> Optional[float]:\n",
    "            s = (s or \"\").strip()\n",
    "            if not s:\n",
    "                return None\n",
    "            if s.startswith(\".\") and s[1:].replace(\".\", \"\", 1).isdigit():\n",
    "                s = \"0\" + s\n",
    "            try:\n",
    "                return float(s)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # keep only per-team rows (exclude TOT/2TM/3TM/4TM)\n",
    "        team_rows = []\n",
    "        for tr in latest_rows:\n",
    "            tds = tr.xpath(\"./td[@data-stat='team_name_abbr']\")\n",
    "            team_txt = tds[0].xpath(\"normalize-space(string(.))\").upper() if tds else \"\"\n",
    "            if team_txt not in aggregate_markers:\n",
    "                team_rows.append(tr)\n",
    "\n",
    "        parsed_rows: List[Dict[str, Optional[float]]] = []\n",
    "        for tr in team_rows:\n",
    "            row_dict: Dict[str, Optional[float]] = {}\n",
    "            # mp weight\n",
    "            mp_td = tr.xpath(\"./td[@data-stat='mp']\")\n",
    "            mp_val = mp_td[0].xpath(\"normalize-space(string(.))\") if mp_td else \"\"\n",
    "            row_dict[\"mp\"] = _to_float_inline(mp_val) or 0.0\n",
    "            # parse candidates\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                td = tr.xpath(f\"./td[@data-stat='{stat}']\")\n",
    "                sval = td[0].xpath(\"normalize-space(string(.))\") if td else \"\"\n",
    "                row_dict[stat] = _to_float_inline(sval)\n",
    "            parsed_rows.append(row_dict)\n",
    "\n",
    "        total_mp = sum((r.get(\"mp\", 0.0) or 0.0) for r in parsed_rows)\n",
    "        if total_mp <= 0:\n",
    "            # fallback: first team row (strings)\n",
    "            chosen = team_rows[0]\n",
    "            td_by_stat = {td.get(\"data-stat\"): td for td in chosen.xpath(\"./td[@data-stat]\")}\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                td = td_by_stat.get(stat)\n",
    "                if td is None:\n",
    "                    continue\n",
    "                val = td.xpath(\"normalize-space(string(.))\")\n",
    "                key = stat if stat.endswith(\"_per_poss\") else f\"{stat}_per_poss\"\n",
    "                out[key] = val\n",
    "        else:\n",
    "            # weighted averages\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                num = 0.0\n",
    "                den = 0.0\n",
    "                for r in parsed_rows:\n",
    "                    v = r.get(stat)\n",
    "                    w = r.get(\"mp\", 0.0) or 0.0\n",
    "                    if v is None:\n",
    "                        continue\n",
    "                    num += v * w\n",
    "                    den += w\n",
    "                sval = \"\"\n",
    "                if den > 0:\n",
    "                    agg = num / den\n",
    "                    sval = f\"{agg:.3f}\".rstrip(\"0\").rstrip(\".\")\n",
    "                key = stat if stat.endswith(\"_per_poss\") else f\"{stat}_per_poss\"\n",
    "                out[key] = sval\n",
    "\n",
    "    # ── career row from <tfoot> (robust selection) ────────────────────────────\n",
    "    tf_rows = table.xpath(\".//tfoot/tr\")\n",
    "    career_tr = None\n",
    "    if tf_rows:\n",
    "        # pass 1: exact \"\\d+ Yrs\"\n",
    "        for tr in tf_rows:\n",
    "            th_text = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "            if re.fullmatch(r\"\\d+\\s+Yr(?:s)?\", th_text or \"\"):\n",
    "                career_tr = tr\n",
    "                break\n",
    "        # pass 2: pick the tfoot with the largest \"<n> Yrs\"\n",
    "        if career_tr is None:\n",
    "            best = None\n",
    "            best_years = -1\n",
    "            for tr in tf_rows:\n",
    "                th_text = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "                m = re.search(r\"(\\d+)\\s+Yr(?:s)?\", th_text or \"\")\n",
    "                if m:\n",
    "                    yrs = int(m.group(1))\n",
    "                    if yrs > best_years:\n",
    "                        best_years = yrs\n",
    "                        best = tr\n",
    "            career_tr = best\n",
    "\n",
    "    if career_tr is not None:\n",
    "        td_by_stat_c = {td.get(\"data-stat\"): td for td in career_tr.xpath(\"./td[@data-stat]\")}\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = td_by_stat_c.get(stat)\n",
    "            if td is None:\n",
    "                continue\n",
    "            val = td.xpath(\"normalize-space(string(.))\")\n",
    "            key = stat if stat.endswith(\"_per_poss\") else f\"{stat}_per_poss\"\n",
    "            out[f\"career_{key}\"] = val\n",
    "\n",
    "    return out\n",
    "\n",
    "def advanced_stats(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    prefer_tot: bool = True,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse the 'Advanced' table (id='advanced'):\n",
    "      - Pick the latest season by max <th data-stat=\"year_id\" csk>.\n",
    "      - If multiple rows (player traded), prefer the aggregate row (TOT/2TM/3TM/4TM) when prefer_tot=True.\n",
    "      - If no aggregate row is available, manually aggregate per-team rows:\n",
    "          * minutes-weighted average for rate/percentage metrics\n",
    "          * simple sum for total-like metrics (ows, dws, ws, vorp)\n",
    "      - Exclude identity columns: year_id, age, team_name_abbr, comp_name_abbr, pos, awards\n",
    "      - Append `_adv` to ALL output metric keys.\n",
    "      - Also extract the 'career' row from <tfoot> (the one with 'Yr' or 'Yrs' in header) as `career_<stat>_adv`.\n",
    "      - If no career footer exists, synthesize `career_*_adv` from the latest-season values.\n",
    "    \"\"\"\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    tables = dom.xpath(\"//table[@id='advanced']\")\n",
    "    if not tables:\n",
    "        return None\n",
    "    table = tables[0]\n",
    "\n",
    "    # Header order (left→right)\n",
    "    header_stats: List[str] = []\n",
    "    for th in table.xpath(\".//thead//tr[1]/th\"):\n",
    "        ds = th.get(\"data-stat\")\n",
    "        if ds:\n",
    "            header_stats.append(ds)\n",
    "\n",
    "    # Body rows with a season cell\n",
    "    rows = table.xpath(\".//tbody/tr[th[@data-stat='year_id']]\")\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    # Latest season via csk\n",
    "    seasons: List[tuple[int, Any]] = []\n",
    "    for tr in rows:\n",
    "        th = tr.xpath(\"./th[@data-stat='year_id']\")[0]\n",
    "        csk = th.get(\"csk\")\n",
    "        try:\n",
    "            csk_int = int(csk)\n",
    "        except (TypeError, ValueError):\n",
    "            txt = th.xpath(\"normalize-space(string(.))\")\n",
    "            m = re.search(r\"(\\d{4})(?:-\\d{2})?$\", txt or \"\")\n",
    "            csk_int = int(m.group(1)) if m else 0\n",
    "        seasons.append((csk_int, tr))\n",
    "    max_csk = max(c for c, _ in seasons)\n",
    "    latest_rows = [tr for (c, tr) in seasons if c == max_csk]\n",
    "\n",
    "    # Helpers\n",
    "    def _cell_text(tr, stat: str) -> str:\n",
    "        if stat == \"year_id\":\n",
    "            th = tr.xpath(\"./th[@data-stat='year_id']\")\n",
    "            return th[0].xpath(\"normalize-space(string(.))\") if th else \"\"\n",
    "        td = tr.xpath(f\"./td[@data-stat='{stat}']\")\n",
    "        return td[0].xpath(\"normalize-space(string(.))\") if td else \"\"\n",
    "\n",
    "    def _to_float(s: str) -> Optional[float]:\n",
    "        s = (s or \"\").strip()\n",
    "        if not s:\n",
    "            return None\n",
    "        if s.startswith(\".\") and s[1:].replace(\".\", \"\", 1).isdigit():\n",
    "            s = \"0\" + s\n",
    "        try:\n",
    "            return float(s)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def _out_key(stat: str) -> str:\n",
    "        # every advanced metric gets _adv suffix\n",
    "        return f\"{stat}_adv\"\n",
    "\n",
    "    aggregate_markers = {\"TOT\", \"2TM\", \"3TM\", \"4TM\"}\n",
    "\n",
    "    # identity/metadata to exclude\n",
    "    exclude_stats = {\n",
    "        \"year_id\", \"age\", \"team_name_abbr\", \"comp_name_abbr\",\n",
    "        \"pos\", \"awards\"\n",
    "    }\n",
    "\n",
    "    # BBR season totals to SUM; everything else minutes-weighted\n",
    "    total_like = {\"ows\", \"dws\", \"ws\", \"vorp\"}\n",
    "\n",
    "    # Prefer official aggregate row\n",
    "    chosen = None\n",
    "    if prefer_tot and len(latest_rows) > 1:\n",
    "        for tr in latest_rows:\n",
    "            team_txt = (_cell_text(tr, \"team_name_abbr\") or \"\").upper()\n",
    "            if team_txt in aggregate_markers:\n",
    "                chosen = tr\n",
    "                break\n",
    "\n",
    "    need_manual_aggregate = chosen is None and len(latest_rows) > 1\n",
    "\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # Single/aggregate row path\n",
    "    if not need_manual_aggregate:\n",
    "        if chosen is None:\n",
    "            chosen = latest_rows[0]\n",
    "\n",
    "        td_by_stat = {td.get(\"data-stat\"): td for td in chosen.xpath(\"./td[@data-stat]\")}\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = td_by_stat.get(stat)\n",
    "            if td is None:\n",
    "                continue\n",
    "            out[_out_key(stat)] = td.xpath(\"normalize-space(string(.))\")\n",
    "\n",
    "    else:\n",
    "        # Manual aggregation (minutes-weighted for rates; sum for totals)\n",
    "        team_rows = []\n",
    "        for tr in latest_rows:\n",
    "            team_txt = (_cell_text(tr, \"team_name_abbr\") or \"\").upper()\n",
    "            if team_txt not in aggregate_markers:\n",
    "                team_rows.append(tr)\n",
    "        if not team_rows:\n",
    "            team_rows = [latest_rows[0]]\n",
    "\n",
    "        parsed_rows: List[Dict[str, Optional[float]]] = []\n",
    "        for tr in team_rows:\n",
    "            row: Dict[str, Optional[float]] = {}\n",
    "            row[\"mp\"] = _to_float(_cell_text(tr, \"mp\")) or 0.0\n",
    "            for stat in header_stats:\n",
    "                if stat in exclude_stats:\n",
    "                    continue\n",
    "                row[stat] = _to_float(_cell_text(tr, stat))\n",
    "            parsed_rows.append(row)\n",
    "\n",
    "        total_mp = sum((r.get(\"mp\") or 0.0) for r in parsed_rows)\n",
    "\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "\n",
    "            if stat in total_like:\n",
    "                s = 0.0\n",
    "                have_any = False\n",
    "                for r in parsed_rows:\n",
    "                    v = r.get(stat)\n",
    "                    if v is not None:\n",
    "                        s += v\n",
    "                        have_any = True\n",
    "                out[_out_key(stat)] = (f\"{s:.3f}\".rstrip(\"0\").rstrip(\".\") if have_any else \"\")\n",
    "            else:\n",
    "                if total_mp <= 0:\n",
    "                    out[_out_key(stat)] = _cell_text(team_rows[0], stat)\n",
    "                else:\n",
    "                    num = 0.0\n",
    "                    for r in parsed_rows:\n",
    "                        v = r.get(stat)\n",
    "                        w = r.get(\"mp\") or 0.0\n",
    "                        if v is None:\n",
    "                            continue\n",
    "                        num += v * w\n",
    "                    agg = num / total_mp\n",
    "                    out[_out_key(stat)] = f\"{agg:.3f}\".rstrip(\"0\").rstrip(\".\")\n",
    "\n",
    "    # ── Career from <tfoot> (accept \"1 Yr\" or \"N Yrs\"); else synthesize ──\n",
    "    career_row = None\n",
    "    tf_rows = table.xpath(\".//tfoot/tr\")\n",
    "    if tf_rows:\n",
    "        # pass 1: exact \"N Yr\" or \"N Yrs\"\n",
    "        for tr in tf_rows:\n",
    "            label = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "            if re.fullmatch(r\"\\d+\\s+Yr(?:s)?\", label or \"\"):\n",
    "                career_row = tr\n",
    "                break\n",
    "        # pass 2: pick the tfoot with the largest \"<n> Yr(s)\"\n",
    "        if career_row is None:\n",
    "            best = None\n",
    "            best_years = -1\n",
    "            for tr in tf_rows:\n",
    "                label = tr.xpath(\"normalize-space(./th[@data-stat='year_id'])\")\n",
    "                m = re.search(r\"(\\d+)\\s+Yr(?:s)?\", label or \"\")\n",
    "                if m:\n",
    "                    yrs = int(m.group(1))\n",
    "                    if yrs > best_years:\n",
    "                        best_years = yrs\n",
    "                        best = tr\n",
    "            career_row = best\n",
    "\n",
    "    if career_row is not None:\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            td = career_row.xpath(f\"./td[@data-stat='{stat}']\")\n",
    "            if not td:\n",
    "                continue\n",
    "            val = td[0].xpath(\"normalize-space(string(.))\")\n",
    "            out[f\"career_{stat}_adv\"] = val\n",
    "    else:\n",
    "        # No footer: synthesize career_* from latest-season values we already computed\n",
    "        for stat in header_stats:\n",
    "            if stat in exclude_stats:\n",
    "                continue\n",
    "            key = _out_key(stat)  # e.g., 'ts_pct_adv'\n",
    "            if key in out:\n",
    "                out[f\"career_{stat}_adv\"] = out[key]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def player_salary(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse the Salaries table (id='all_salaries') and return only the current season salary.\n",
    "    Example output:\n",
    "        { \"salary\": \"$17,260,000\" }\n",
    "    \"\"\"\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    table_nodes = dom.xpath(\"//table[@id='all_salaries']\")\n",
    "    if not table_nodes:\n",
    "        return None\n",
    "    table = table_nodes[0]\n",
    "\n",
    "    # Helper: convert \"2024-25\" → 2025\n",
    "    def _season_end_year(season_text: str) -> int:\n",
    "        s = (season_text or \"\").strip()\n",
    "        m = re.match(r\"^(\\d{4})(?:-(\\d{2}))?$\", s)\n",
    "        if not m:\n",
    "            return 0\n",
    "        start_year = int(m.group(1))\n",
    "        return start_year + 1 if m.group(2) else start_year\n",
    "\n",
    "    rows = table.xpath(\".//tbody/tr[th[@data-stat='season']]\")\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    # Pick the latest season row\n",
    "    best = None\n",
    "    best_end_year = -1\n",
    "    for tr in rows:\n",
    "        th = tr.xpath(\"./th[@data-stat='season']\")\n",
    "        season_text = th[0].xpath(\"normalize-space(string(.))\") if th else \"\"\n",
    "        end_year = _season_end_year(season_text)\n",
    "        if end_year > best_end_year:\n",
    "            best_end_year = end_year\n",
    "            best = tr\n",
    "\n",
    "    if best is None:\n",
    "        return None\n",
    "\n",
    "    # Get salary text from that row\n",
    "    sal_td = best.xpath(\"./td[@data-stat='salary']\")\n",
    "    salary_text = sal_td[0].xpath(\"normalize-space(string(.))\") if sal_td else \"\"\n",
    "    return {\"salary\": salary_text}\n",
    "\n",
    "\n",
    "\n",
    "def player_current_contract(\n",
    "    url: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    dom: Optional[html.HtmlElement] = None,\n",
    "    html_text: Optional[str] = None,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Current contract salary from a contracts_* table:\n",
    "      - Find first contracts_* table (e.g., contracts_lac, contracts_pho)\n",
    "      - Take the FIRST row\n",
    "      - Skip the Team cell, read the FIRST salary cell\n",
    "      - Return {'current_contract': '$X,XXX,XXX'} or blank if unavailable\n",
    "    \"\"\"\n",
    "    # Ensure we have a DOM that already unhides commented tables\n",
    "    if dom is None:\n",
    "        if html_text is None:\n",
    "            html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "        dom = _build_dom(html_text)\n",
    "\n",
    "    # 1) DOM path: any table with id beginning with \"contracts_\"\n",
    "    tables = dom.xpath(\"//table[starts-with(@id,'contracts_')]\")\n",
    "    table = tables[0] if tables else None\n",
    "\n",
    "    # 2) Fallback: regex scan raw HTML (first contracts_* table)\n",
    "    if table is None and html_text:\n",
    "        m = re.search(r\"(<table[^>]+id=[\\\"']?contracts_[^>]*>.*?</table>)\",\n",
    "                      html_text, re.DOTALL | re.IGNORECASE)\n",
    "        if m:\n",
    "            try:\n",
    "                table = html.fromstring(m.group(1))\n",
    "            except Exception:\n",
    "                table = None\n",
    "\n",
    "    # Nothing found → blank\n",
    "    if table is None:\n",
    "        return {\"current_contract\": \"\"}\n",
    "\n",
    "    # Find the first data row\n",
    "    rows = table.xpath(\".//tbody/tr\") or table.xpath(\".//tr[td]\")\n",
    "    if not rows:\n",
    "        return {\"current_contract\": \"\"}\n",
    "\n",
    "    first_row = rows[0]\n",
    "\n",
    "    # Cells: [0]=Team, [1]=first season salary (the one we want)\n",
    "    tds = first_row.xpath(\"./td\")\n",
    "    if len(tds) < 2:\n",
    "        return {\"current_contract\": \"\"}\n",
    "\n",
    "    # Read the first salary cell (skip the Team cell)\n",
    "    salary_text = tds[1].xpath(\"normalize-space(string(.))\") or \"\"\n",
    "    return {\"current_contract\": salary_text}\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# URL reader (absolute path per your setup) + CSV writer (preserve order)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _read_urls_from_file(\n",
    "    txt_path: str = \"data/players.txt\"\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads player URLs from file and normalizes them.\n",
    "    Accepts:\n",
    "      - full URLs (http/https, with/without www, with ?/# → stripped)\n",
    "      - paths like /players/t/tatumja01[.html]\n",
    "      - bare ids like tatumja01  (infer folder)\n",
    "    De-duplicates while preserving order.\n",
    "    Prints diagnostics + shows duplicates & invalid entries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ players file not found: {txt_path}\")\n",
    "        return []\n",
    "\n",
    "    def _normalize(token: str) -> Optional[str]:\n",
    "        token = token.strip()\n",
    "        if not token:\n",
    "            return None\n",
    "        token = token.split(\"#\", 1)[0].strip()  # remove inline comments\n",
    "        if not token:\n",
    "            return None\n",
    "\n",
    "        # bare id like 'tatumja01'\n",
    "        if re.fullmatch(r\"[A-Za-z][A-Za-z0-9]{8,}\", token):\n",
    "            pid = token.lower()\n",
    "            return f\"https://www.basketball-reference.com/players/{pid[0]}/{pid}.html\"\n",
    "\n",
    "        # path like /players/t/tatumja01\n",
    "        if re.fullmatch(r\"/?players/[A-Za-z]/[A-Za-z0-9]+(?:\\.html)?/?\", token):\n",
    "            path = token if token.startswith(\"/\") else \"/\" + token\n",
    "            if not path.endswith(\".html\"):\n",
    "                path = path.rstrip(\"/\") + \".html\"\n",
    "            parts = path.split(\"/\")\n",
    "            parts[-1] = parts[-1].lower()\n",
    "            parts[-2] = parts[-2].lower()\n",
    "            return \"https://www.basketball-reference.com\" + \"/\".join(parts)\n",
    "\n",
    "        # URL without scheme\n",
    "        if token.startswith(\"www.basketball-reference.com/\"):\n",
    "            token = \"https://\" + token\n",
    "\n",
    "        # full URL\n",
    "        if token.startswith((\"http://\", \"https://\")):\n",
    "            m = re.match(r\"^(https?://)(?:www\\.)?basketball-reference\\.com([^?#]*)\", token, re.I)\n",
    "            if not m:\n",
    "                return None\n",
    "            path = m.group(2)\n",
    "            if not path.startswith(\"/\"):\n",
    "                path = \"/\" + path\n",
    "            m2 = re.match(r\"^/players/([A-Za-z])/([A-Za-z0-9]+)(?:\\.html)?/?$\", path)\n",
    "            if not m2:\n",
    "                return None\n",
    "            return f\"https://www.basketball-reference.com/players/{m2.group(1).lower()}/{m2.group(2).lower()}.html\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    tokens = re.split(r\"[\\s,]+\", raw.strip())\n",
    "    normalized, invalid_samples = [], []\n",
    "    total_tokens = 0\n",
    "    for t in tokens:\n",
    "        if not t:\n",
    "            continue\n",
    "        total_tokens += 1\n",
    "        u = _normalize(t)\n",
    "        if u:\n",
    "            normalized.append(u)\n",
    "        else:\n",
    "            if len(invalid_samples) < 10:\n",
    "                invalid_samples.append(t)\n",
    "\n",
    "    # de-dup but capture duplicates\n",
    "    seen, unique_urls, duplicates = set(), [], []\n",
    "    for u in normalized:\n",
    "        if u in seen:\n",
    "            duplicates.append(u)\n",
    "        else:\n",
    "            seen.add(u)\n",
    "            unique_urls.append(u)\n",
    "\n",
    "    # diagnostics\n",
    "    num_valid = len(normalized)\n",
    "    num_unique = len(unique_urls)\n",
    "    num_dupes = len(duplicates)\n",
    "    num_invalid = total_tokens - num_valid\n",
    "\n",
    "    print(\n",
    "        f\"🔎 Parsed {total_tokens} entries → valid {num_valid}, \"\n",
    "        f\"duplicates {num_dupes}, invalid {num_invalid}.\"\n",
    "    )\n",
    "    print(f\"📄 Using {num_unique} unique URL(s) from {txt_path}\")\n",
    "\n",
    "    if duplicates:\n",
    "        print(\"⚠️ Duplicate entries (player appeared multiple times):\")\n",
    "        for d in duplicates[:20]:  # limit to first 20\n",
    "            print(f\"   - {d}\")\n",
    "        if len(duplicates) > 20:\n",
    "            print(f\"   ... and {len(duplicates)-20} more\")\n",
    "\n",
    "    if invalid_samples:\n",
    "        print(\"⚠️ Sample invalid entries (first 10):\")\n",
    "        for s in invalid_samples:\n",
    "            print(f\"   - {s}\")\n",
    "\n",
    "    return unique_urls\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Runner (logging + CSV)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def basketball_ref_all_players_stats(\n",
    "    urls: Optional[List[str]] = None,\n",
    "    api_key: Optional[str] = None,\n",
    "    csv_path: str = \"output_data/all_nba_players_stats.csv\",\n",
    "    prefer_tot: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Scrape players and stream results to CSV row-by-row, combining:\n",
    "      - players_personal_info()\n",
    "      - player_salary()              -> 'salary' (current-season *paid* salary table)\n",
    "      - current_contract_salary()    -> 'current_contract_salary' (contracts table for detected season)\n",
    "      - per_game_stats()\n",
    "      - per_36_min_stats()\n",
    "      - per_100_poss_stats()\n",
    "      - advanced_stats()\n",
    "    \"\"\"\n",
    "    if urls is None:\n",
    "        urls = _read_urls_from_file()\n",
    "\n",
    "    if not urls:\n",
    "        print(\"🚫 No player URLs to process. Provide `urls=[...]` or put them in data/players.txt\")\n",
    "        return\n",
    "\n",
    "    total = len(urls)\n",
    "\n",
    "    _ensure_dir(os.path.dirname(csv_path) or \".\")\n",
    "    fieldnames = None\n",
    "    file = open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\")\n",
    "    writer = None\n",
    "\n",
    "    try:\n",
    "        for i, url in enumerate(urls, 1):\n",
    "            try:\n",
    "                html_text = fetch_html_via_scraperapi(url, api_key=api_key)\n",
    "                dom = _build_dom(html_text)\n",
    "\n",
    "                personal  = players_personal_info(url, api_key=api_key, dom=dom, html_text=html_text)\n",
    "                salary    = player_salary(url, api_key=api_key, dom=dom, html_text=html_text)                  # {'salary': '...'}\n",
    "                contract = player_current_contract(url, api_key=api_key, dom=dom, html_text=html_text)   # {'current_contract_salary': '...'}\n",
    "                pergame   = per_game_stats(url, api_key=api_key, dom=dom, html_text=html_text)\n",
    "                per36     = per_36_min_stats(url, api_key=api_key, prefer_tot=prefer_tot, dom=dom, html_text=html_text)\n",
    "                per100    = per_100_poss_stats(url, api_key=api_key, prefer_tot=prefer_tot, dom=dom, html_text=html_text)\n",
    "                advanced  = advanced_stats(url, api_key=api_key, prefer_tot=prefer_tot, dom=dom, html_text=html_text)\n",
    "\n",
    "                # Merge row\n",
    "                combined = dict(personal)\n",
    "                if salary:\n",
    "                    combined.update(salary)              # adds 'salary'\n",
    "                if contract:\n",
    "                    combined.update(contract)            # adds 'current_contract'\n",
    "                if pergame:\n",
    "                    combined.update(pergame)\n",
    "                if per36:\n",
    "                    combined.update(per36)\n",
    "                if per100:\n",
    "                    combined.update(per100)\n",
    "                if advanced:\n",
    "                    combined.update(advanced)\n",
    "\n",
    "                combined.setdefault(\"url\", url)\n",
    "\n",
    "                # Initialize header order once (keep your established order)\n",
    "                if fieldnames is None:\n",
    "                    personal_first = [\n",
    "                        \"player_id\",\"player_name\",\"team\",\"birth_day\",\"years_experience\",\n",
    "                        \"salary\",                     # from player_salary()\n",
    "                        \"current_contract\",    # from current_contract()\n",
    "                        \"season\",\"age\",\"team_id\",\"lg_id\",\"pos\",\"g\",\"gs\",\n",
    "                        \"was_traded\",\"teams_count\",\"teams_played\"\n",
    "                    ]\n",
    "\n",
    "                    pergame_cols = []\n",
    "                    if pergame:\n",
    "                        for k in pergame.keys():\n",
    "                            if k not in personal_first:\n",
    "                                pergame_cols.append(k)\n",
    "\n",
    "                    per36_cols     = list(per36.keys()) if per36 else []\n",
    "                    per100_cols    = list(per100.keys()) if per100 else []\n",
    "                    advanced_cols  = list(advanced.keys()) if advanced else []\n",
    "\n",
    "                    fieldnames = personal_first + pergame_cols + per36_cols + per100_cols + advanced_cols\n",
    "\n",
    "                    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "\n",
    "                # Write row\n",
    "                writer.writerow({k: combined.get(k, \"\") for k in fieldnames})\n",
    "                file.flush()\n",
    "\n",
    "                print(f\"[{i}/{total}] ✅ {combined.get('player_id')} — written\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{i}/{total}] 💥 Error for {url}: {e}\")\n",
    "\n",
    "            # time.sleep(random.uniform(0.2, 0.6))\n",
    "    finally:\n",
    "        file.close()\n",
    "\n",
    "    print(f\"📝 Finished. CSV written to {csv_path}\")\n",
    "\n",
    "               \n",
    "if __name__ == \"__main__\":\n",
    "    basketball_ref_all_players_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b3788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
